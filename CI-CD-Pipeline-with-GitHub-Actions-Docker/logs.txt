
==> Audit <==
|------------|--------------------------------|----------|---------|---------|---------------------|---------------------|
|  Command   |              Args              | Profile  |  User   | Version |     Start Time      |      End Time       |
|------------|--------------------------------|----------|---------|---------|---------------------|---------------------|
| start      | --driver=docker                | minikube | akash_k | v1.35.0 | 20 Apr 25 11:41 UTC | 20 Apr 25 11:45 UTC |
| dashboard  |                                | minikube | akash_k | v1.35.0 | 20 Apr 25 11:48 UTC |                     |
| dashboard  |                                | minikube | akash_k | v1.35.0 | 20 Apr 25 11:52 UTC |                     |
| service    | nginx-service --url            | minikube | akash_k | v1.35.0 | 20 Apr 25 12:49 UTC | 20 Apr 25 12:50 UTC |
| start      |                                | minikube | akash_k | v1.35.0 | 29 Apr 25 07:21 UTC | 29 Apr 25 07:21 UTC |
| service    | todo-app-service --url         | minikube | akash_k | v1.35.0 | 29 Apr 25 07:23 UTC |                     |
| service    | todo-app-7d84777d8c-jhbvl      | minikube | akash_k | v1.35.0 | 29 Apr 25 07:26 UTC |                     |
|            | --url                          |          |         |         |                     |                     |
| service    | todo-app-service --url         | minikube | akash_k | v1.35.0 | 29 Apr 25 07:27 UTC |                     |
| docker-env |                                | minikube | akash_k | v1.35.0 | 29 Apr 25 07:34 UTC | 29 Apr 25 07:34 UTC |
| service    | todo-app-service --url         | minikube | akash_k | v1.35.0 | 29 Apr 25 07:36 UTC |                     |
| start      |                                | minikube | akash_k | v1.35.0 | 29 Apr 25 07:36 UTC | 29 Apr 25 07:36 UTC |
| service    | todo-app-service --url         | minikube | akash_k | v1.35.0 | 29 Apr 25 07:38 UTC |                     |
| docker-env |                                | minikube | akash_k | v1.35.0 | 29 Apr 25 07:39 UTC | 29 Apr 25 07:39 UTC |
| service    | todo-app-service --url         | minikube | akash_k | v1.35.0 | 29 Apr 25 07:43 UTC |                     |
|------------|--------------------------------|----------|---------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/04/29 07:36:31
Running on machine: MSI
Binary: Built with gc go1.23.4 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0429 07:36:31.560654    5742 out.go:345] Setting OutFile to fd 1 ...
I0429 07:36:31.560753    5742 out.go:397] isatty.IsTerminal(1) = true
I0429 07:36:31.560756    5742 out.go:358] Setting ErrFile to fd 2...
I0429 07:36:31.560758    5742 out.go:397] isatty.IsTerminal(2) = true
I0429 07:36:31.560891    5742 root.go:338] Updating PATH: /home/akash_k/.minikube/bin
I0429 07:36:31.560917    5742 oci.go:582] shell is pointing to dockerd inside minikube. will unset to use host
W0429 07:36:31.560981    5742 root.go:314] Error reading config file at /home/akash_k/.minikube/config/config.json: open /home/akash_k/.minikube/config/config.json: no such file or directory
I0429 07:36:31.561329    5742 out.go:352] Setting JSON to false
I0429 07:36:31.562483    5742 start.go:129] hostinfo: {"hostname":"MSI","uptime":4883,"bootTime":1745907309,"procs":42,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"24.04","kernelVersion":"5.15.167.4-microsoft-standard-WSL2","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"guest","hostId":"2a9194b4-38cd-4ee2-a468-531a9640fd7a"}
I0429 07:36:31.562514    5742 start.go:139] virtualization:  guest
I0429 07:36:31.564264    5742 out.go:177] 😄  minikube v1.35.0 on Ubuntu 24.04 (amd64)
I0429 07:36:31.566766    5742 notify.go:220] Checking for updates...
I0429 07:36:31.566785    5742 out.go:177]     ▪ MINIKUBE_ACTIVE_DOCKERD=minikube
I0429 07:36:31.568715    5742 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0429 07:36:31.569451    5742 driver.go:394] Setting default libvirt URI to qemu:///system
I0429 07:36:31.629360    5742 docker.go:123] docker version: linux-28.0.4:Docker Desktop 4.0.0 ()
I0429 07:36:31.629414    5742 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0429 07:36:32.798844    5742 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (1.169406638s)
I0429 07:36:32.799176    5742 info.go:266] docker info: {ID:450bc1d2-47ee-4902-97b6-3ffa789b1c9e Containers:17 ContainersRunning:1 ContainersPaused:0 ContainersStopped:16 Images:6 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:84 OomKillDisable:true NGoroutines:95 SystemTime:2025-04-29 07:36:32.791834272 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:18 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:16 MemTotal:8175042560 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///var/run/docker-cli.sock] ExperimentalBuild:false ServerVersion:28.0.4 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:753481ec61c7c8955a23d6ff7bc8e4daed455734 Expected:753481ec61c7c8955a23d6ff7bc8e4daed455734} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:v1.2.5-0-g59923ef} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: DOCKER_INSECURE_NO_IPTABLES_RAW is set WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:/usr/local/lib/docker/cli-plugins/docker-ai SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.1.3] map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.22.0-desktop.1] map[Name:cloud Path:/usr/local/lib/docker/cli-plugins/docker-cloud SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:0.2.20] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.34.0-desktop.1] map[Name:debug Path:/usr/local/lib/docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:/usr/local/lib/docker/cli-plugins/docker-desktop SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Beta) Vendor:Docker Inc. Version:v0.1.6] map[Name:dev Path:/usr/local/lib/docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:/usr/local/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:init Path:/usr/local/lib/docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:sbom Path:/usr/local/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/usr/local/lib/docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.17.0]] Warnings:<nil>}}
I0429 07:36:32.799295    5742 docker.go:318] overlay module found
I0429 07:36:32.801127    5742 out.go:177] ✨  Using the docker driver based on existing profile
I0429 07:36:32.802788    5742 start.go:297] selected driver: docker
I0429 07:36:32.802810    5742 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/akash_k:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0429 07:36:32.802851    5742 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0429 07:36:32.802914    5742 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0429 07:36:32.966036    5742 info.go:266] docker info: {ID:450bc1d2-47ee-4902-97b6-3ffa789b1c9e Containers:17 ContainersRunning:1 ContainersPaused:0 ContainersStopped:16 Images:6 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:84 OomKillDisable:true NGoroutines:95 SystemTime:2025-04-29 07:36:32.959453277 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:18 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:16 MemTotal:8175042560 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///var/run/docker-cli.sock] ExperimentalBuild:false ServerVersion:28.0.4 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:753481ec61c7c8955a23d6ff7bc8e4daed455734 Expected:753481ec61c7c8955a23d6ff7bc8e4daed455734} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:v1.2.5-0-g59923ef} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: DOCKER_INSECURE_NO_IPTABLES_RAW is set WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:/usr/local/lib/docker/cli-plugins/docker-ai SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.1.3] map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.22.0-desktop.1] map[Name:cloud Path:/usr/local/lib/docker/cli-plugins/docker-cloud SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:0.2.20] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.34.0-desktop.1] map[Name:debug Path:/usr/local/lib/docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:/usr/local/lib/docker/cli-plugins/docker-desktop SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Beta) Vendor:Docker Inc. Version:v0.1.6] map[Name:dev Path:/usr/local/lib/docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:/usr/local/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:init Path:/usr/local/lib/docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:sbom Path:/usr/local/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/usr/local/lib/docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.17.0]] Warnings:<nil>}}
I0429 07:36:32.966534    5742 cni.go:84] Creating CNI manager for ""
I0429 07:36:32.966562    5742 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0429 07:36:32.966604    5742 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/akash_k:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0429 07:36:32.968357    5742 out.go:177] 👍  Starting "minikube" primary control-plane node in "minikube" cluster
I0429 07:36:32.969750    5742 cache.go:121] Beginning downloading kic base image for docker with docker
I0429 07:36:32.971297    5742 out.go:177] 🚜  Pulling base image v0.0.46 ...
I0429 07:36:32.972956    5742 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0429 07:36:32.972981    5742 preload.go:146] Found local preload: /home/akash_k/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4
I0429 07:36:32.972985    5742 cache.go:56] Caching tarball of preloaded images
I0429 07:36:32.973031    5742 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon
I0429 07:36:32.973071    5742 preload.go:172] Found /home/akash_k/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0429 07:36:32.973076    5742 cache.go:59] Finished verifying existence of preloaded tar for v1.32.0 on docker
I0429 07:36:32.973131    5742 profile.go:143] Saving config to /home/akash_k/.minikube/profiles/minikube/config.json ...
I0429 07:36:32.999632    5742 image.go:100] Found gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon, skipping pull
I0429 07:36:32.999641    5742 cache.go:145] gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 exists in daemon, skipping load
I0429 07:36:32.999659    5742 cache.go:227] Successfully downloaded all kic artifacts
I0429 07:36:32.999676    5742 start.go:360] acquireMachinesLock for minikube: {Name:mka47c074f9679341f75839b29fd8b75a5cdd3ab Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0429 07:36:32.999726    5742 start.go:364] duration metric: took 40.101µs to acquireMachinesLock for "minikube"
I0429 07:36:32.999734    5742 start.go:96] Skipping create...Using existing machine configuration
I0429 07:36:32.999754    5742 fix.go:54] fixHost starting: 
I0429 07:36:33.001591    5742 out.go:177] 📌  Noticed you have an activated docker-env on docker driver in this terminal:
W0429 07:36:33.003289    5742 out.go:270] ❗  Please re-eval your docker-env, To ensure your environment variables have updated ports:

	'minikube -p minikube docker-env'

	
I0429 07:36:33.003351    5742 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0429 07:36:33.020159    5742 fix.go:112] recreateIfNeeded on minikube: state=Running err=<nil>
W0429 07:36:33.020174    5742 fix.go:138] unexpected machine state, will restart: <nil>
I0429 07:36:33.021865    5742 out.go:177] 🏃  Updating the running docker "minikube" container ...
I0429 07:36:33.023315    5742 machine.go:93] provisionDockerMachine start ...
I0429 07:36:33.023362    5742 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0429 07:36:33.039689    5742 main.go:141] libmachine: Using SSH client type: native
I0429 07:36:33.039835    5742 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 64265 <nil> <nil>}
I0429 07:36:33.039839    5742 main.go:141] libmachine: About to run SSH command:
hostname
I0429 07:36:33.154012    5742 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0429 07:36:33.154024    5742 ubuntu.go:169] provisioning hostname "minikube"
I0429 07:36:33.154060    5742 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0429 07:36:33.175394    5742 main.go:141] libmachine: Using SSH client type: native
I0429 07:36:33.175496    5742 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 64265 <nil> <nil>}
I0429 07:36:33.175500    5742 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0429 07:36:33.313043    5742 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0429 07:36:33.313170    5742 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0429 07:36:33.330056    5742 main.go:141] libmachine: Using SSH client type: native
I0429 07:36:33.330170    5742 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 64265 <nil> <nil>}
I0429 07:36:33.330178    5742 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0429 07:36:33.442835    5742 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0429 07:36:33.442882    5742 ubuntu.go:175] set auth options {CertDir:/home/akash_k/.minikube CaCertPath:/home/akash_k/.minikube/certs/ca.pem CaPrivateKeyPath:/home/akash_k/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/akash_k/.minikube/machines/server.pem ServerKeyPath:/home/akash_k/.minikube/machines/server-key.pem ClientKeyPath:/home/akash_k/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/akash_k/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/akash_k/.minikube}
I0429 07:36:33.442900    5742 ubuntu.go:177] setting up certificates
I0429 07:36:33.442905    5742 provision.go:84] configureAuth start
I0429 07:36:33.442944    5742 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0429 07:36:33.458708    5742 provision.go:143] copyHostCerts
I0429 07:36:33.458763    5742 exec_runner.go:144] found /home/akash_k/.minikube/ca.pem, removing ...
I0429 07:36:33.458773    5742 exec_runner.go:203] rm: /home/akash_k/.minikube/ca.pem
I0429 07:36:33.460690    5742 exec_runner.go:151] cp: /home/akash_k/.minikube/certs/ca.pem --> /home/akash_k/.minikube/ca.pem (1078 bytes)
I0429 07:36:33.460927    5742 exec_runner.go:144] found /home/akash_k/.minikube/cert.pem, removing ...
I0429 07:36:33.460932    5742 exec_runner.go:203] rm: /home/akash_k/.minikube/cert.pem
I0429 07:36:33.460961    5742 exec_runner.go:151] cp: /home/akash_k/.minikube/certs/cert.pem --> /home/akash_k/.minikube/cert.pem (1123 bytes)
I0429 07:36:33.461143    5742 exec_runner.go:144] found /home/akash_k/.minikube/key.pem, removing ...
I0429 07:36:33.461146    5742 exec_runner.go:203] rm: /home/akash_k/.minikube/key.pem
I0429 07:36:33.461165    5742 exec_runner.go:151] cp: /home/akash_k/.minikube/certs/key.pem --> /home/akash_k/.minikube/key.pem (1679 bytes)
I0429 07:36:33.461358    5742 provision.go:117] generating server cert: /home/akash_k/.minikube/machines/server.pem ca-key=/home/akash_k/.minikube/certs/ca.pem private-key=/home/akash_k/.minikube/certs/ca-key.pem org=akash_k.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0429 07:36:33.562300    5742 provision.go:177] copyRemoteCerts
I0429 07:36:33.562332    5742 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0429 07:36:33.562351    5742 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0429 07:36:33.577489    5742 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:64265 SSHKeyPath:/home/akash_k/.minikube/machines/minikube/id_rsa Username:docker}
I0429 07:36:33.521587    5742 ssh_runner.go:362] scp /home/akash_k/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0429 07:36:33.535086    5742 ssh_runner.go:362] scp /home/akash_k/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1078 bytes)
I0429 07:36:33.547342    5742 ssh_runner.go:362] scp /home/akash_k/.minikube/machines/server.pem --> /etc/docker/server.pem (1184 bytes)
I0429 07:36:33.562041    5742 provision.go:87] duration metric: took 274.737819ms to configureAuth
I0429 07:36:33.562053    5742 ubuntu.go:193] setting minikube options for container-runtime
I0429 07:36:33.562141    5742 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0429 07:36:33.562167    5742 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0429 07:36:33.580142    5742 main.go:141] libmachine: Using SSH client type: native
I0429 07:36:33.580300    5742 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 64265 <nil> <nil>}
I0429 07:36:33.580305    5742 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0429 07:36:33.698427    5742 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0429 07:36:33.698436    5742 ubuntu.go:71] root file system type: overlay
I0429 07:36:33.698702    5742 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0429 07:36:33.698741    5742 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0429 07:36:33.717370    5742 main.go:141] libmachine: Using SSH client type: native
I0429 07:36:33.717464    5742 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 64265 <nil> <nil>}
I0429 07:36:33.717492    5742 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0429 07:36:33.833739    5742 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0429 07:36:33.833777    5742 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0429 07:36:33.852676    5742 main.go:141] libmachine: Using SSH client type: native
I0429 07:36:33.852769    5742 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 64265 <nil> <nil>}
I0429 07:36:33.852777    5742 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0429 07:36:33.971391    5742 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0429 07:36:33.971403    5742 machine.go:96] duration metric: took 1.103691079s to provisionDockerMachine
I0429 07:36:33.971409    5742 start.go:293] postStartSetup for "minikube" (driver="docker")
I0429 07:36:33.971415    5742 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0429 07:36:33.971445    5742 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0429 07:36:33.971468    5742 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0429 07:36:33.990146    5742 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:64265 SSHKeyPath:/home/akash_k/.minikube/machines/minikube/id_rsa Username:docker}
I0429 07:36:34.080352    5742 ssh_runner.go:195] Run: cat /etc/os-release
I0429 07:36:34.082241    5742 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0429 07:36:34.082252    5742 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0429 07:36:34.082256    5742 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0429 07:36:34.082259    5742 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0429 07:36:34.082315    5742 filesync.go:126] Scanning /home/akash_k/.minikube/addons for local assets ...
I0429 07:36:34.083743    5742 filesync.go:126] Scanning /home/akash_k/.minikube/files for local assets ...
I0429 07:36:34.084164    5742 start.go:296] duration metric: took 112.748505ms for postStartSetup
I0429 07:36:34.084196    5742 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0429 07:36:34.084215    5742 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0429 07:36:34.105301    5742 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:64265 SSHKeyPath:/home/akash_k/.minikube/machines/minikube/id_rsa Username:docker}
I0429 07:36:34.191113    5742 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0429 07:36:34.194020    5742 fix.go:56] duration metric: took 1.349871409s for fixHost
I0429 07:36:34.194029    5742 start.go:83] releasing machines lock for "minikube", held for 1.349909688s
I0429 07:36:34.194067    5742 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0429 07:36:34.209918    5742 ssh_runner.go:195] Run: cat /version.json
I0429 07:36:34.209940    5742 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0429 07:36:34.209994    5742 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0429 07:36:34.210028    5742 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0429 07:36:34.229250    5742 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:64265 SSHKeyPath:/home/akash_k/.minikube/machines/minikube/id_rsa Username:docker}
I0429 07:36:34.230315    5742 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:64265 SSHKeyPath:/home/akash_k/.minikube/machines/minikube/id_rsa Username:docker}
I0429 07:36:34.588591    5742 ssh_runner.go:195] Run: systemctl --version
I0429 07:36:34.593890    5742 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0429 07:36:34.596702    5742 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0429 07:36:34.609013    5742 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0429 07:36:34.609057    5742 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0429 07:36:34.613948    5742 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0429 07:36:34.613960    5742 start.go:495] detecting cgroup driver to use...
I0429 07:36:34.613979    5742 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0429 07:36:34.614187    5742 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0429 07:36:34.623039    5742 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0429 07:36:34.628522    5742 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0429 07:36:34.633759    5742 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0429 07:36:34.633782    5742 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0429 07:36:34.638845    5742 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0429 07:36:34.643767    5742 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0429 07:36:34.648648    5742 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0429 07:36:34.653390    5742 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0429 07:36:34.658612    5742 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0429 07:36:34.663565    5742 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0429 07:36:34.668542    5742 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0429 07:36:34.673689    5742 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0429 07:36:34.678326    5742 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0429 07:36:34.682962    5742 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0429 07:36:34.802887    5742 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0429 07:36:45.266186    5742 ssh_runner.go:235] Completed: sudo systemctl restart containerd: (10.463267719s)
I0429 07:36:45.266207    5742 start.go:495] detecting cgroup driver to use...
I0429 07:36:45.266238    5742 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0429 07:36:45.266307    5742 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0429 07:36:45.273612    5742 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0429 07:36:45.273643    5742 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0429 07:36:45.281360    5742 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0429 07:36:45.290973    5742 ssh_runner.go:195] Run: which cri-dockerd
I0429 07:36:45.293218    5742 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0429 07:36:45.298207    5742 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0429 07:36:45.308588    5742 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0429 07:36:45.390284    5742 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0429 07:36:45.475238    5742 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0429 07:36:45.475412    5742 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0429 07:36:45.491021    5742 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0429 07:36:45.579160    5742 ssh_runner.go:195] Run: sudo systemctl restart docker
I0429 07:36:49.563236    5742 ssh_runner.go:235] Completed: sudo systemctl restart docker: (3.984056626s)
I0429 07:36:49.563349    5742 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0429 07:36:49.571779    5742 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0429 07:36:49.581822    5742 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0429 07:36:49.589417    5742 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0429 07:36:49.647086    5742 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0429 07:36:49.727142    5742 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0429 07:36:49.807303    5742 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0429 07:36:49.815628    5742 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0429 07:36:49.822682    5742 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0429 07:36:49.900862    5742 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0429 07:36:49.958276    5742 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0429 07:36:49.958345    5742 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0429 07:36:49.960592    5742 start.go:563] Will wait 60s for crictl version
I0429 07:36:49.960617    5742 ssh_runner.go:195] Run: which crictl
I0429 07:36:49.962576    5742 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0429 07:36:50.047355    5742 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.4.1
RuntimeApiVersion:  v1
I0429 07:36:50.047386    5742 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0429 07:36:50.128781    5742 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0429 07:36:50.149157    5742 out.go:235] 🐳  Preparing Kubernetes v1.32.0 on Docker 27.4.1 ...
I0429 07:36:50.149293    5742 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0429 07:36:50.170854    5742 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0429 07:36:50.173392    5742 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0429 07:36:50.193896    5742 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/akash_k:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0429 07:36:50.194047    5742 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0429 07:36:50.194076    5742 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0429 07:36:50.211514    5742 docker.go:689] Got preloaded images: -- stdout --
akash5507/todo-app:latest
nginx:latest
nginx:<none>
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0429 07:36:50.211522    5742 docker.go:619] Images already preloaded, skipping extraction
I0429 07:36:50.211569    5742 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0429 07:36:50.227646    5742 docker.go:689] Got preloaded images: -- stdout --
akash5507/todo-app:latest
nginx:latest
nginx:<none>
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0429 07:36:50.227688    5742 cache_images.go:84] Images are preloaded, skipping loading
I0429 07:36:50.227696    5742 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.32.0 docker true true} ...
I0429 07:36:50.227844    5742 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.32.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0429 07:36:50.227879    5742 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0429 07:36:50.389043    5742 cni.go:84] Creating CNI manager for ""
I0429 07:36:50.389064    5742 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0429 07:36:50.389105    5742 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0429 07:36:50.389145    5742 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.32.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0429 07:36:50.389252    5742 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.32.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0429 07:36:50.389305    5742 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.32.0
I0429 07:36:50.396442    5742 binaries.go:44] Found k8s binaries, skipping transfer
I0429 07:36:50.396476    5742 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0429 07:36:50.402535    5742 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0429 07:36:50.413549    5742 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0429 07:36:50.425268    5742 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2286 bytes)
I0429 07:36:50.437277    5742 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0429 07:36:50.439646    5742 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0429 07:36:50.528494    5742 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0429 07:36:50.535595    5742 certs.go:68] Setting up /home/akash_k/.minikube/profiles/minikube for IP: 192.168.49.2
I0429 07:36:50.535601    5742 certs.go:194] generating shared ca certs ...
I0429 07:36:50.535610    5742 certs.go:226] acquiring lock for ca certs: {Name:mk58a02d8b9c1da75a0b4f472ed390cd719accb3 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0429 07:36:50.535767    5742 certs.go:235] skipping valid "minikubeCA" ca cert: /home/akash_k/.minikube/ca.key
I0429 07:36:50.536908    5742 certs.go:235] skipping valid "proxyClientCA" ca cert: /home/akash_k/.minikube/proxy-client-ca.key
I0429 07:36:50.536923    5742 certs.go:256] generating profile certs ...
I0429 07:36:50.537000    5742 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": /home/akash_k/.minikube/profiles/minikube/client.key
I0429 07:36:50.537236    5742 certs.go:359] skipping valid signed profile cert regeneration for "minikube": /home/akash_k/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I0429 07:36:50.537422    5742 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": /home/akash_k/.minikube/profiles/minikube/proxy-client.key
I0429 07:36:50.537681    5742 certs.go:484] found cert: /home/akash_k/.minikube/certs/ca-key.pem (1679 bytes)
I0429 07:36:50.537694    5742 certs.go:484] found cert: /home/akash_k/.minikube/certs/ca.pem (1078 bytes)
I0429 07:36:50.537704    5742 certs.go:484] found cert: /home/akash_k/.minikube/certs/cert.pem (1123 bytes)
I0429 07:36:50.537712    5742 certs.go:484] found cert: /home/akash_k/.minikube/certs/key.pem (1679 bytes)
I0429 07:36:50.538494    5742 ssh_runner.go:362] scp /home/akash_k/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0429 07:36:50.552167    5742 ssh_runner.go:362] scp /home/akash_k/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0429 07:36:50.565877    5742 ssh_runner.go:362] scp /home/akash_k/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0429 07:36:50.579540    5742 ssh_runner.go:362] scp /home/akash_k/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0429 07:36:50.593745    5742 ssh_runner.go:362] scp /home/akash_k/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0429 07:36:50.610533    5742 ssh_runner.go:362] scp /home/akash_k/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0429 07:36:50.626677    5742 ssh_runner.go:362] scp /home/akash_k/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0429 07:36:50.672495    5742 ssh_runner.go:362] scp /home/akash_k/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0429 07:36:50.689129    5742 ssh_runner.go:362] scp /home/akash_k/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0429 07:36:50.766516    5742 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0429 07:36:50.784214    5742 ssh_runner.go:195] Run: openssl version
I0429 07:36:50.865866    5742 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0429 07:36:50.874982    5742 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0429 07:36:50.877794    5742 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Apr 20 11:45 /usr/share/ca-certificates/minikubeCA.pem
I0429 07:36:50.877831    5742 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0429 07:36:50.885415    5742 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0429 07:36:50.891703    5742 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0429 07:36:50.894163    5742 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0429 07:36:50.955868    5742 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0429 07:36:50.964056    5742 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0429 07:36:50.969011    5742 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0429 07:36:50.974647    5742 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0429 07:36:50.979381    5742 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0429 07:36:50.986794    5742 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/akash_k:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0429 07:36:50.987099    5742 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0429 07:36:51.070209    5742 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0429 07:36:51.078346    5742 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I0429 07:36:51.078382    5742 kubeadm.go:593] restartPrimaryControlPlane start ...
I0429 07:36:51.078424    5742 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0429 07:36:51.155602    5742 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0429 07:36:51.155648    5742 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0429 07:36:51.185984    5742 kubeconfig.go:125] found "minikube" server: "https://127.0.0.1:64264"
I0429 07:36:51.188350    5742 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0429 07:36:51.261829    5742 kubeadm.go:630] The running cluster does not require reconfiguration: 127.0.0.1
I0429 07:36:51.262100    5742 kubeadm.go:597] duration metric: took 183.709723ms to restartPrimaryControlPlane
I0429 07:36:51.262107    5742 kubeadm.go:394] duration metric: took 275.322719ms to StartCluster
I0429 07:36:51.262120    5742 settings.go:142] acquiring lock: {Name:mk46f1c3aa438226867e2f8be95f94f9029250d2 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0429 07:36:51.262168    5742 settings.go:150] Updating kubeconfig:  /home/akash_k/.kube/config
I0429 07:36:51.265434    5742 lock.go:35] WriteFile acquiring /home/akash_k/.kube/config: {Name:mk569b82f018fe6f49184eb69050d1056bdcf8f4 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0429 07:36:51.265626    5742 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0429 07:36:51.265725    5742 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0429 07:36:51.265778    5742 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0429 07:36:51.265797    5742 addons.go:238] Setting addon storage-provisioner=true in "minikube"
W0429 07:36:51.265809    5742 addons.go:247] addon storage-provisioner should already be in state true
I0429 07:36:51.265811    5742 addons.go:69] Setting dashboard=true in profile "minikube"
I0429 07:36:51.265812    5742 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0429 07:36:51.265826    5742 addons.go:238] Setting addon dashboard=true in "minikube"
W0429 07:36:51.265832    5742 addons.go:247] addon dashboard should already be in state true
I0429 07:36:51.265836    5742 host.go:66] Checking if "minikube" exists ...
I0429 07:36:51.265837    5742 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0429 07:36:51.265852    5742 host.go:66] Checking if "minikube" exists ...
I0429 07:36:51.265857    5742 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0429 07:36:51.266058    5742 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0429 07:36:51.266082    5742 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0429 07:36:51.266134    5742 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0429 07:36:51.268035    5742 out.go:177] 🔎  Verifying Kubernetes components...
I0429 07:36:51.271568    5742 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0429 07:36:51.296472    5742 out.go:177]     ▪ Using image docker.io/kubernetesui/dashboard:v2.7.0
I0429 07:36:51.296595    5742 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0429 07:36:51.297726    5742 addons.go:238] Setting addon default-storageclass=true in "minikube"
W0429 07:36:51.297753    5742 addons.go:247] addon default-storageclass should already be in state true
I0429 07:36:51.297769    5742 host.go:66] Checking if "minikube" exists ...
I0429 07:36:51.297953    5742 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0429 07:36:51.298098    5742 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0429 07:36:51.298104    5742 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0429 07:36:51.298131    5742 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0429 07:36:51.299509    5742 out.go:177]     ▪ Using image docker.io/kubernetesui/metrics-scraper:v1.0.8
I0429 07:36:51.300924    5742 addons.go:435] installing /etc/kubernetes/addons/dashboard-ns.yaml
I0429 07:36:51.300933    5742 ssh_runner.go:362] scp dashboard/dashboard-ns.yaml --> /etc/kubernetes/addons/dashboard-ns.yaml (759 bytes)
I0429 07:36:51.300990    5742 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0429 07:36:51.318330    5742 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:64265 SSHKeyPath:/home/akash_k/.minikube/machines/minikube/id_rsa Username:docker}
I0429 07:36:51.323843    5742 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0429 07:36:51.323855    5742 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0429 07:36:51.323893    5742 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0429 07:36:51.327394    5742 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:64265 SSHKeyPath:/home/akash_k/.minikube/machines/minikube/id_rsa Username:docker}
I0429 07:36:51.343502    5742 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:64265 SSHKeyPath:/home/akash_k/.minikube/machines/minikube/id_rsa Username:docker}
I0429 07:36:51.658791    5742 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0429 07:36:51.669458    5742 addons.go:435] installing /etc/kubernetes/addons/dashboard-clusterrole.yaml
I0429 07:36:51.669476    5742 ssh_runner.go:362] scp dashboard/dashboard-clusterrole.yaml --> /etc/kubernetes/addons/dashboard-clusterrole.yaml (1001 bytes)
I0429 07:36:51.757684    5742 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0429 07:36:51.757766    5742 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0429 07:36:51.769460    5742 addons.go:435] installing /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml
I0429 07:36:51.769491    5742 ssh_runner.go:362] scp dashboard/dashboard-clusterrolebinding.yaml --> /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml (1018 bytes)
I0429 07:36:51.769520    5742 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0429 07:36:51.785817    5742 api_server.go:52] waiting for apiserver process to appear ...
I0429 07:36:51.785858    5742 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0429 07:36:51.861014    5742 addons.go:435] installing /etc/kubernetes/addons/dashboard-configmap.yaml
I0429 07:36:51.861029    5742 ssh_runner.go:362] scp dashboard/dashboard-configmap.yaml --> /etc/kubernetes/addons/dashboard-configmap.yaml (837 bytes)
I0429 07:36:51.959992    5742 addons.go:435] installing /etc/kubernetes/addons/dashboard-dp.yaml
I0429 07:36:51.960001    5742 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-dp.yaml (4288 bytes)
I0429 07:36:51.980390    5742 addons.go:435] installing /etc/kubernetes/addons/dashboard-role.yaml
I0429 07:36:51.980402    5742 ssh_runner.go:362] scp dashboard/dashboard-role.yaml --> /etc/kubernetes/addons/dashboard-role.yaml (1724 bytes)
I0429 07:36:52.067650    5742 addons.go:435] installing /etc/kubernetes/addons/dashboard-rolebinding.yaml
I0429 07:36:52.067685    5742 ssh_runner.go:362] scp dashboard/dashboard-rolebinding.yaml --> /etc/kubernetes/addons/dashboard-rolebinding.yaml (1046 bytes)
I0429 07:36:52.164366    5742 addons.go:435] installing /etc/kubernetes/addons/dashboard-sa.yaml
I0429 07:36:52.164377    5742 ssh_runner.go:362] scp dashboard/dashboard-sa.yaml --> /etc/kubernetes/addons/dashboard-sa.yaml (837 bytes)
I0429 07:36:52.178882    5742 addons.go:435] installing /etc/kubernetes/addons/dashboard-secret.yaml
I0429 07:36:52.178896    5742 ssh_runner.go:362] scp dashboard/dashboard-secret.yaml --> /etc/kubernetes/addons/dashboard-secret.yaml (1389 bytes)
I0429 07:36:52.276503    5742 addons.go:435] installing /etc/kubernetes/addons/dashboard-svc.yaml
I0429 07:36:52.276514    5742 ssh_runner.go:362] scp dashboard/dashboard-svc.yaml --> /etc/kubernetes/addons/dashboard-svc.yaml (1294 bytes)
I0429 07:36:52.375052    5742 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0429 07:36:53.070702    5742 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.312996678s)
W0429 07:36:53.070724    5742 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0429 07:36:53.070800    5742 retry.go:31] will retry after 182.244576ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0429 07:36:53.070875    5742 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (1.301346501s)
W0429 07:36:53.070889    5742 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0429 07:36:53.070896    5742 retry.go:31] will retry after 159.904142ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0429 07:36:53.070913    5742 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (1.285047397s)
I0429 07:36:53.070951    5742 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0429 07:36:53.071054    5742 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0429 07:36:53.071065    5742 retry.go:31] will retry after 322.296918ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0429 07:36:53.168763    5742 api_server.go:72] duration metric: took 1.90311889s to wait for apiserver process to appear ...
I0429 07:36:53.168773    5742 api_server.go:88] waiting for apiserver healthz status ...
I0429 07:36:53.168785    5742 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:64264/healthz ...
I0429 07:36:53.172826    5742 api_server.go:269] stopped: https://127.0.0.1:64264/healthz: Get "https://127.0.0.1:64264/healthz": read tcp 127.0.0.1:41530->127.0.0.1:64264: read: connection reset by peer
I0429 07:36:53.231119    5742 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0429 07:36:53.253725    5742 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0429 07:36:53.394096    5742 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0429 07:36:53.669793    5742 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:64264/healthz ...
I0429 07:36:55.282109    5742 api_server.go:279] https://127.0.0.1:64264/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0429 07:36:55.282130    5742 api_server.go:103] status: https://127.0.0.1:64264/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0429 07:36:55.282150    5742 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:64264/healthz ...
I0429 07:36:55.365663    5742 api_server.go:279] https://127.0.0.1:64264/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[-]poststarthook/start-service-ip-repair-controllers failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0429 07:36:55.365679    5742 api_server.go:103] status: https://127.0.0.1:64264/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[-]poststarthook/start-service-ip-repair-controllers failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0429 07:36:55.669585    5742 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:64264/healthz ...
I0429 07:36:55.672652    5742 api_server.go:279] https://127.0.0.1:64264/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0429 07:36:55.672665    5742 api_server.go:103] status: https://127.0.0.1:64264/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0429 07:36:56.169385    5742 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:64264/healthz ...
I0429 07:36:56.171976    5742 api_server.go:279] https://127.0.0.1:64264/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0429 07:36:56.171985    5742 api_server.go:103] status: https://127.0.0.1:64264/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0429 07:36:56.194412    5742 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (2.940668985s)
I0429 07:36:56.194467    5742 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (2.963327232s)
I0429 07:36:56.669671    5742 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:64264/healthz ...
I0429 07:36:56.672939    5742 api_server.go:279] https://127.0.0.1:64264/healthz returned 200:
ok
I0429 07:36:56.674041    5742 api_server.go:141] control plane version: v1.32.0
I0429 07:36:56.674055    5742 api_server.go:131] duration metric: took 3.505277379s to wait for apiserver health ...
I0429 07:36:56.674104    5742 system_pods.go:43] waiting for kube-system pods to appear ...
I0429 07:36:56.679746    5742 system_pods.go:59] 7 kube-system pods found
I0429 07:36:56.679771    5742 system_pods.go:61] "coredns-668d6bf9bc-5slr9" [113128c1-b43c-4555-ad92-5364510a23fb] Running
I0429 07:36:56.679775    5742 system_pods.go:61] "etcd-minikube" [75780eaa-6582-4b67-a6f8-cfe1134eadbc] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0429 07:36:56.679781    5742 system_pods.go:61] "kube-apiserver-minikube" [def6a732-cf06-4102-a68c-e42b63debf9a] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0429 07:36:56.679784    5742 system_pods.go:61] "kube-controller-manager-minikube" [9be0ad00-2dfd-48d2-88b5-e41b5f78d977] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0429 07:36:56.679786    5742 system_pods.go:61] "kube-proxy-t8kjv" [511fb777-6950-4286-ba59-cfa5154c25ba] Running
I0429 07:36:56.679788    5742 system_pods.go:61] "kube-scheduler-minikube" [2062a575-8cbe-4097-8f11-7f7463fb04f9] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0429 07:36:56.679789    5742 system_pods.go:61] "storage-provisioner" [38616fb2-4571-4d8b-b779-17fae6845def] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0429 07:36:56.679792    5742 system_pods.go:74] duration metric: took 5.684649ms to wait for pod list to return data ...
I0429 07:36:56.679798    5742 kubeadm.go:582] duration metric: took 5.414160046s to wait for: map[apiserver:true system_pods:true]
I0429 07:36:56.679805    5742 node_conditions.go:102] verifying NodePressure condition ...
I0429 07:36:56.682484    5742 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0429 07:36:56.682494    5742 node_conditions.go:123] node cpu capacity is 16
I0429 07:36:56.682500    5742 node_conditions.go:105] duration metric: took 2.692718ms to run NodePressure ...
I0429 07:36:56.682507    5742 start.go:241] waiting for startup goroutines ...
I0429 07:36:56.701374    5742 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: (3.307252608s)
I0429 07:36:56.703066    5742 out.go:177] 💡  Some dashboard features require the metrics-server addon. To enable all features please run:

	minikube addons enable metrics-server

I0429 07:36:56.705112    5742 out.go:177] 🌟  Enabled addons: storage-provisioner, default-storageclass, dashboard
I0429 07:36:56.706728    5742 addons.go:514] duration metric: took 5.441031159s for enable addons: enabled=[storage-provisioner default-storageclass dashboard]
I0429 07:36:56.706775    5742 start.go:246] waiting for cluster config update ...
I0429 07:36:56.706785    5742 start.go:255] writing updated cluster config ...
I0429 07:36:56.707134    5742 ssh_runner.go:195] Run: rm -f paused
I0429 07:36:56.908210    5742 start.go:600] kubectl: 1.33.0, cluster: 1.32.0 (minor skew: 1)
I0429 07:36:56.909764    5742 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Apr 29 07:36:49 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Apr 29 07:36:49 minikube systemd[1]: cri-docker.service: Deactivated successfully.
Apr 29 07:36:49 minikube systemd[1]: Stopped CRI Interface for Docker Application Container Engine.
Apr 29 07:36:49 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Apr 29 07:36:49 minikube cri-dockerd[9399]: time="2025-04-29T07:36:49Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Apr 29 07:36:49 minikube cri-dockerd[9399]: time="2025-04-29T07:36:49Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Apr 29 07:36:49 minikube cri-dockerd[9399]: time="2025-04-29T07:36:49Z" level=info msg="Start docker client with request timeout 0s"
Apr 29 07:36:49 minikube cri-dockerd[9399]: time="2025-04-29T07:36:49Z" level=info msg="Hairpin mode is set to hairpin-veth"
Apr 29 07:36:49 minikube cri-dockerd[9399]: time="2025-04-29T07:36:49Z" level=info msg="Loaded network plugin cni"
Apr 29 07:36:49 minikube cri-dockerd[9399]: time="2025-04-29T07:36:49Z" level=info msg="Docker cri networking managed by network plugin cni"
Apr 29 07:36:49 minikube cri-dockerd[9399]: time="2025-04-29T07:36:49Z" level=info msg="Setting cgroupDriver cgroupfs"
Apr 29 07:36:49 minikube cri-dockerd[9399]: time="2025-04-29T07:36:49Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Apr 29 07:36:49 minikube cri-dockerd[9399]: time="2025-04-29T07:36:49Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Apr 29 07:36:49 minikube cri-dockerd[9399]: time="2025-04-29T07:36:49Z" level=info msg="Start cri-dockerd grpc backend"
Apr 29 07:36:49 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Apr 29 07:36:50 minikube cri-dockerd[9399]: time="2025-04-29T07:36:50Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-668d6bf9bc-5slr9_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"89f463b482365a932e4a8f5ad2cd5568cd87d31ad7fbb57f5501577d651bd82a\""
Apr 29 07:36:50 minikube cri-dockerd[9399]: time="2025-04-29T07:36:50Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-668d6bf9bc-5slr9_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"d9c979a67df1fb4748b4c18f31d4d846374b205a6ea8776c8e73f71701148329\""
Apr 29 07:36:50 minikube cri-dockerd[9399]: time="2025-04-29T07:36:50Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"nginx-deployment-96b9d695-4t4zd_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"35d50227b0300aab8d3d7ba507308a0adaf6693730bef793e2a8257b35a7ba06\""
Apr 29 07:36:50 minikube cri-dockerd[9399]: time="2025-04-29T07:36:50Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"nginx-deployment-96b9d695-4t4zd_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"0403f954bd54e6ef8c359df241e4ea6b6400e95b64925415a699dbe19930d3b3\""
Apr 29 07:36:50 minikube cri-dockerd[9399]: time="2025-04-29T07:36:50Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"todo-app-7d84777d8c-jhbvl_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"dbb89c227cb8edbea426515b62d4271a0fd2c09a36a3585f0220369f90dc7e74\""
Apr 29 07:36:50 minikube cri-dockerd[9399]: time="2025-04-29T07:36:50Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"nginx-deployment-96b9d695-96fxr_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"ba98172894e1e9903c7033ba12cfed611a1e241369615dbbdf34b2a0f0ef4998\""
Apr 29 07:36:50 minikube cri-dockerd[9399]: time="2025-04-29T07:36:50Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"nginx-deployment-96b9d695-96fxr_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"722765f5f2474589f32c152d6858854d0131f21895f1b47ff749553e9750a1a3\""
Apr 29 07:36:50 minikube cri-dockerd[9399]: time="2025-04-29T07:36:50Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"nginx-deployment-96b9d695-99679_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"465cb40412eb75cc3f8a1e98cb688e282625f6dea533d7f060d8103981a97194\""
Apr 29 07:36:50 minikube cri-dockerd[9399]: time="2025-04-29T07:36:50Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"nginx-deployment-96b9d695-99679_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"810b261ebe3fcdf3c159773d804039cf9a31dff7f513072216ed74701a5f3dfd\""
Apr 29 07:36:50 minikube cri-dockerd[9399]: time="2025-04-29T07:36:50Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"dashboard-metrics-scraper-5d59dccf9b-sxd7q_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"85ba866edfcabb908ae54b7fbd8cc79bea900f0607aed95f3fc95e7eeeb7efcd\""
Apr 29 07:36:50 minikube cri-dockerd[9399]: time="2025-04-29T07:36:50Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"dashboard-metrics-scraper-5d59dccf9b-sxd7q_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"abae0a4546bec8016ca3f33ad682d8432c2da678d0a6dfe89fedd56eea8c69c5\""
Apr 29 07:36:50 minikube cri-dockerd[9399]: time="2025-04-29T07:36:50Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"kubernetes-dashboard-7779f9b69b-4ql4n_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"4f84cbe397cd05c1b08eada481696bf2ee8e7f4f8ff74cdf624dd301a2b8f08d\""
Apr 29 07:36:50 minikube cri-dockerd[9399]: time="2025-04-29T07:36:50Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/eff039f975506d2cf3ae46003f0d8c15b64bc4990886619f99fe6fa0458adb00/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Apr 29 07:36:51 minikube cri-dockerd[9399]: time="2025-04-29T07:36:51Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2de1d9e2cd9867aeeb8fea7905e83c5bb633b1d2d1393bb84e340e54bc3f9004/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Apr 29 07:36:51 minikube cri-dockerd[9399]: time="2025-04-29T07:36:51Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/90ae5e73c3eaf9166b27707a9b46e6e5552c0bb6c43ac90afb739d84e0861f45/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Apr 29 07:36:51 minikube cri-dockerd[9399]: time="2025-04-29T07:36:51Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e9004d7439cc945e1d1a8dc08f170e70df1b24140be33f57fa0d4f6404dfb806/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Apr 29 07:36:51 minikube cri-dockerd[9399]: time="2025-04-29T07:36:51Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/576fe128706cfd74a494d9ccc880c5d750ea3b2cd131e99a587fd1e1a1fa67da/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Apr 29 07:36:51 minikube cri-dockerd[9399]: time="2025-04-29T07:36:51Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a71cee81422568deb7e59e6342b7f7db470b86c082a57a2c3aee04ba227a6d24/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Apr 29 07:36:51 minikube dockerd[9047]: time="2025-04-29T07:36:51.958907468Z" level=info msg="ignoring event" container=dea086a0a41e914df5df4e64caef0e6f7cf6d5c9eb2f943806c29e6abaa553b4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 29 07:36:52 minikube cri-dockerd[9399]: time="2025-04-29T07:36:52Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/b87601b13f10d8d4cf585b7384c13b4d84c9ef4c2b0e673f8230edd37fa72f21/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 29 07:36:52 minikube cri-dockerd[9399]: time="2025-04-29T07:36:52Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7d34db1a7425d7fa41d3ff64b12f341168090c11864e0aa6f389555704bb8de5/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Apr 29 07:36:52 minikube cri-dockerd[9399]: time="2025-04-29T07:36:52Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5877071dac0205bebee60a7fcc7bf01b3218f0dc6b6f9d1bb7f6490d87488c09/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 29 07:36:53 minikube cri-dockerd[9399]: time="2025-04-29T07:36:53Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/51e44c56fcaddb52b1e24b08fa21b022228a41e249a9fc3df1fceaac92de5545/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 29 07:36:53 minikube cri-dockerd[9399]: time="2025-04-29T07:36:53Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c36f5690556bb09851da18f5ca450e23619b8cef8235d47f5aa4c508c167992a/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 29 07:36:53 minikube cri-dockerd[9399]: time="2025-04-29T07:36:53Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/88f8fb3a0c65672b7c609323359ad89c7c3f2e5cf748ce2b7c213982682e4c00/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 29 07:36:53 minikube cri-dockerd[9399]: time="2025-04-29T07:36:53Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/daeca9b3dead422a2bf41bc8c8692b0209ffc132d6edca2af03d460fbf18951e/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 29 07:36:56 minikube cri-dockerd[9399]: time="2025-04-29T07:36:56Z" level=info msg="Stop pulling image nginx:latest: Status: Image is up to date for nginx:latest"
Apr 29 07:36:58 minikube cri-dockerd[9399]: time="2025-04-29T07:36:58Z" level=info msg="Stop pulling image nginx:latest: Status: Image is up to date for nginx:latest"
Apr 29 07:37:01 minikube cri-dockerd[9399]: time="2025-04-29T07:37:01Z" level=info msg="Stop pulling image nginx:latest: Status: Image is up to date for nginx:latest"
Apr 29 07:37:02 minikube cri-dockerd[9399]: time="2025-04-29T07:37:02Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d4d8258d4f3d1eccfbaa8a899b25be005077bc329a3eb198494e67efcfb6f756/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 29 07:37:04 minikube cri-dockerd[9399]: time="2025-04-29T07:37:04Z" level=info msg="Stop pulling image nginx:latest: Status: Image is up to date for nginx:latest"
Apr 29 07:39:41 minikube dockerd[9047]: time="2025-04-29T07:39:41.873810676Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 29 07:39:41 minikube dockerd[9047]: time="2025-04-29T07:39:41.873880738Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 29 07:40:33 minikube dockerd[9047]: time="2025-04-29T07:40:33.400880445Z" level=info msg="ignoring event" container=b87601b13f10d8d4cf585b7384c13b4d84c9ef4c2b0e673f8230edd37fa72f21 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 29 07:41:40 minikube cri-dockerd[9399]: time="2025-04-29T07:41:40Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ad84879f47f8ce7fe9c5e8ba044597a1a86c67c89d95da7eaeba12086d65546c/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 29 07:41:43 minikube dockerd[9047]: time="2025-04-29T07:41:43.228046012Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 29 07:41:43 minikube dockerd[9047]: time="2025-04-29T07:41:43.228089079Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 29 07:41:50 minikube dockerd[9047]: time="2025-04-29T07:41:50.966822093Z" level=info msg="ignoring event" container=ad84879f47f8ce7fe9c5e8ba044597a1a86c67c89d95da7eaeba12086d65546c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 29 07:42:57 minikube cri-dockerd[9399]: time="2025-04-29T07:42:57Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/20d2c8dca4cb102947181b34f067ef85befb64072d5d7e3582fc8fb9b303b19c/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 29 07:43:00 minikube dockerd[9047]: time="2025-04-29T07:43:00.617118407Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 29 07:43:00 minikube dockerd[9047]: time="2025-04-29T07:43:00.617248449Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 29 07:43:17 minikube dockerd[9047]: time="2025-04-29T07:43:17.645317511Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 29 07:43:17 minikube dockerd[9047]: time="2025-04-29T07:43:17.645355727Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 29 07:43:48 minikube dockerd[9047]: time="2025-04-29T07:43:48.511815460Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 29 07:43:48 minikube dockerd[9047]: time="2025-04-29T07:43:48.511854311Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"


==> container status <==
CONTAINER           IMAGE                                                                           CREATED             STATE               NAME                        ATTEMPT             POD ID              POD
a94645fe20694       6e38f40d628db                                                                   7 minutes ago       Running             storage-provisioner         5                   2de1d9e2cd986       storage-provisioner
d5e768ec97833       nginx@sha256:c15da6c91de8d2f436196f3a768483ad32c258ed4e1beb3d367a27ed67253e66   7 minutes ago       Running             nginx                       2                   d4d8258d4f3d1       nginx-deployment-96b9d695-65zhm
276f5b74e5b89       nginx@sha256:c15da6c91de8d2f436196f3a768483ad32c258ed4e1beb3d367a27ed67253e66   7 minutes ago       Running             nginx                       2                   c36f5690556bb       nginx-deployment-96b9d695-99679
9481146f7548e       nginx@sha256:c15da6c91de8d2f436196f3a768483ad32c258ed4e1beb3d367a27ed67253e66   7 minutes ago       Running             nginx                       2                   51e44c56fcadd       nginx-deployment-96b9d695-4t4zd
0a38381004836       nginx@sha256:c15da6c91de8d2f436196f3a768483ad32c258ed4e1beb3d367a27ed67253e66   7 minutes ago       Running             nginx                       2                   5877071dac020       nginx-deployment-96b9d695-96fxr
29383be35f748       115053965e86b                                                                   7 minutes ago       Running             dashboard-metrics-scraper   2                   daeca9b3dead4       dashboard-metrics-scraper-5d59dccf9b-sxd7q
bbcb6f8b86f3b       07655ddf2eebe                                                                   7 minutes ago       Running             kubernetes-dashboard        3                   88f8fb3a0c656       kubernetes-dashboard-7779f9b69b-4ql4n
f5385d2b08bae       c69fa2e9cbf5f                                                                   7 minutes ago       Running             coredns                     2                   7d34db1a7425d       coredns-668d6bf9bc-5slr9
0bbedf9173d52       8cab3d2a8bd0f                                                                   7 minutes ago       Running             kube-controller-manager     2                   a71cee8142256       kube-controller-manager-minikube
8657334eb8f0c       c2e17b8d0f4a3                                                                   7 minutes ago       Running             kube-apiserver              2                   576fe128706cf       kube-apiserver-minikube
e058a0d39561b       a9e7e6b294baf                                                                   7 minutes ago       Running             etcd                        2                   e9004d7439cc9       etcd-minikube
5d20a6cba9b78       a389e107f4ff1                                                                   7 minutes ago       Running             kube-scheduler              2                   90ae5e73c3eaf       kube-scheduler-minikube
dea086a0a41e9       6e38f40d628db                                                                   7 minutes ago       Exited              storage-provisioner         4                   2de1d9e2cd986       storage-provisioner
4ba95890a4558       040f9f8aac8cd                                                                   7 minutes ago       Running             kube-proxy                  2                   eff039f975506       kube-proxy-t8kjv
4ec195fe43d9e       07655ddf2eebe                                                                   22 minutes ago      Exited              kubernetes-dashboard        2                   4f84cbe397cd0       kubernetes-dashboard-7779f9b69b-4ql4n
aab729d890ab8       nginx@sha256:c15da6c91de8d2f436196f3a768483ad32c258ed4e1beb3d367a27ed67253e66   22 minutes ago      Exited              nginx                       1                   35d50227b0300       nginx-deployment-96b9d695-4t4zd
b197eb16981d7       nginx@sha256:c15da6c91de8d2f436196f3a768483ad32c258ed4e1beb3d367a27ed67253e66   22 minutes ago      Exited              nginx                       1                   11358f40dc291       nginx-deployment-96b9d695-65zhm
7d83d749cac53       nginx@sha256:c15da6c91de8d2f436196f3a768483ad32c258ed4e1beb3d367a27ed67253e66   22 minutes ago      Exited              nginx                       1                   ba98172894e1e       nginx-deployment-96b9d695-96fxr
9c20235f66541       nginx@sha256:c15da6c91de8d2f436196f3a768483ad32c258ed4e1beb3d367a27ed67253e66   22 minutes ago      Exited              nginx                       1                   465cb40412eb7       nginx-deployment-96b9d695-99679
deeb4f122fb87       c69fa2e9cbf5f                                                                   22 minutes ago      Exited              coredns                     1                   89f463b482365       coredns-668d6bf9bc-5slr9
00340e3d6c44b       115053965e86b                                                                   22 minutes ago      Exited              dashboard-metrics-scraper   1                   85ba866edfcab       dashboard-metrics-scraper-5d59dccf9b-sxd7q
fd91d60f25cec       040f9f8aac8cd                                                                   22 minutes ago      Exited              kube-proxy                  1                   66e588b8a30da       kube-proxy-t8kjv
ef441cc2f8c16       c2e17b8d0f4a3                                                                   22 minutes ago      Exited              kube-apiserver              1                   1433a9b51c3e0       kube-apiserver-minikube
a3596411d9f22       8cab3d2a8bd0f                                                                   22 minutes ago      Exited              kube-controller-manager     1                   dbcd297498c94       kube-controller-manager-minikube
22862f449c781       a9e7e6b294baf                                                                   22 minutes ago      Exited              etcd                        1                   6c33707f578ab       etcd-minikube
17352b3dce2e2       a389e107f4ff1                                                                   22 minutes ago      Exited              kube-scheduler              1                   3ff3105c7b741       kube-scheduler-minikube


==> coredns [deeb4f122fb8] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 9e2996f8cb67ac53e0259ab1f8d615d07d1beb0bd07e6a1e39769c3bf486a905bb991cc47f8d2f14d0d3a90a87dfc625a0b4c524fed169d8158c40657c0694b1
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9
[INFO] 127.0.0.1:59654 - 28817 "HINFO IN 8502823550383046276.7339768203888021611. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.268830654s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[288947640]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (29-Apr-2025 07:21:24.991) (total time: 21125ms):
Trace[288947640]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21125ms (07:21:45.966)
Trace[288947640]: [21.125650567s] [21.125650567s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[1700683045]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (29-Apr-2025 07:21:24.990) (total time: 21141ms):
Trace[1700683045]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21141ms (07:21:45.982)
Trace[1700683045]: [21.141917949s] [21.141917949s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[1613011812]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (29-Apr-2025 07:21:24.990) (total time: 21141ms):
Trace[1613011812]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21141ms (07:21:45.982)
Trace[1613011812]: [21.141793392s] [21.141793392s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> coredns [f5385d2b08ba] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
.:53
[INFO] plugin/reload: Running configuration SHA512 = 9e2996f8cb67ac53e0259ab1f8d615d07d1beb0bd07e6a1e39769c3bf486a905bb991cc47f8d2f14d0d3a90a87dfc625a0b4c524fed169d8158c40657c0694b1
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9
[INFO] 127.0.0.1:36576 - 3139 "HINFO IN 740093834573820350.6931934063665300501. udp 56 false 512" NXDOMAIN qr,rd,ra 56 0.196436644s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_04_20T11_45_35_0700
                    minikube.k8s.io/version=v1.35.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sun, 20 Apr 2025 11:45:32 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Tue, 29 Apr 2025 07:44:14 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Tue, 29 Apr 2025 07:42:10 +0000   Sun, 20 Apr 2025 11:45:32 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Tue, 29 Apr 2025 07:42:10 +0000   Sun, 20 Apr 2025 11:45:32 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Tue, 29 Apr 2025 07:42:10 +0000   Sun, 20 Apr 2025 11:45:32 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Tue, 29 Apr 2025 07:42:10 +0000   Sun, 20 Apr 2025 11:45:33 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                16
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7983440Ki
  pods:               110
Allocatable:
  cpu:                16
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7983440Ki
  pods:               110
System Info:
  Machine ID:                 6fe444d638fb46a6a5b264c10efc93ce
  System UUID:                6fe444d638fb46a6a5b264c10efc93ce
  Boot ID:                    aaeaff3b-7867-489c-a69b-c7b1fd04f196
  Kernel Version:             5.15.167.4-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.4.1
  Kubelet Version:            v1.32.0
  Kube-Proxy Version:         v1.32.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (14 in total)
  Namespace                   Name                                          CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                          ------------  ----------  ---------------  -------------  ---
  default                     nginx-deployment-96b9d695-4t4zd               0 (0%)        0 (0%)      0 (0%)           0 (0%)         8d
  default                     nginx-deployment-96b9d695-65zhm               0 (0%)        0 (0%)      0 (0%)           0 (0%)         8d
  default                     nginx-deployment-96b9d695-96fxr               0 (0%)        0 (0%)      0 (0%)           0 (0%)         8d
  default                     nginx-deployment-96b9d695-99679               0 (0%)        0 (0%)      0 (0%)           0 (0%)         8d
  default                     todo-app-7d84777d8c-pddwn                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         81s
  kube-system                 coredns-668d6bf9bc-5slr9                      100m (0%)     0 (0%)      70Mi (0%)        170Mi (2%)     8d
  kube-system                 etcd-minikube                                 100m (0%)     0 (0%)      100Mi (1%)       0 (0%)         8d
  kube-system                 kube-apiserver-minikube                       250m (1%)     0 (0%)      0 (0%)           0 (0%)         8d
  kube-system                 kube-controller-manager-minikube              200m (1%)     0 (0%)      0 (0%)           0 (0%)         8d
  kube-system                 kube-proxy-t8kjv                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         8d
  kube-system                 kube-scheduler-minikube                       100m (0%)     0 (0%)      0 (0%)           0 (0%)         8d
  kube-system                 storage-provisioner                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         8d
  kubernetes-dashboard        dashboard-metrics-scraper-5d59dccf9b-sxd7q    0 (0%)        0 (0%)      0 (0%)           0 (0%)         8d
  kubernetes-dashboard        kubernetes-dashboard-7779f9b69b-4ql4n         0 (0%)        0 (0%)      0 (0%)           0 (0%)         8d
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (4%)   0 (0%)
  memory             170Mi (2%)  170Mi (2%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                             Age                From             Message
  ----     ------                             ----               ----             -------
  Normal   Starting                           22m                kube-proxy       
  Normal   Starting                           7m23s              kube-proxy       
  Warning  PossibleMemoryBackedVolumesOnDisk  22m                kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   Starting                           22m                kubelet          Starting kubelet.
  Warning  CgroupV1                           22m                kubelet          cgroup v1 support is in maintenance mode, please migrate to cgroup v2
  Normal   NodeHasSufficientMemory            22m (x8 over 22m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              22m (x8 over 22m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               22m (x7 over 22m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced            22m                kubelet          Updated Node Allocatable limit across pods
  Warning  Rebooted                           22m                kubelet          Node minikube has been rebooted, boot id: aaeaff3b-7867-489c-a69b-c7b1fd04f196
  Normal   RegisteredNode                     22m                node-controller  Node minikube event: Registered Node minikube in Controller
  Normal   RegisteredNode                     7m20s              node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[  +0.000314] netlink: 'init': attribute type 4 has an invalid length.
[Apr29 06:44] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.001226] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000282] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000254] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000334] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.003408] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000418] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000252] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000407] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +1.779625] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.001711] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000459] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000401] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000485] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.004874] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001161] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000880] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000654] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[Apr29 06:45] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.002265] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000547] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000488] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000813] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.005368] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000959] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000659] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001593] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[ +20.369109] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.001649] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000371] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000379] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000416] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.002673] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000365] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000274] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000399] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[Apr29 06:56] blk_update_request: I/O error, dev sde, sector 0 op 0x1:(WRITE) flags 0x800 phys_seg 0 prio class 0
[  +8.479357] WSL (1) ERROR: ConfigApplyWindowsLibPath:2531: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000005]  failed 2
[  +0.008611] WSL (1) WARNING: /usr/share/zoneinfo/Asia/Calcutta not found. Is the tzdata package installed?
[  +0.124548] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.001604] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000635] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000640] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001004] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.003450] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000639] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000560] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000593] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.116243] Buffer I/O error on dev sde, logical block 134184960, lost sync page write
[  +0.000824] JBD2: Error -5 detected when updating journal superblock for sde-8.
[  +0.000557] Aborting journal on device sde-8.
[  +0.000403] Buffer I/O error on dev sde, logical block 134184960, lost sync page write
[  +0.000529] JBD2: Error -5 detected when updating journal superblock for sde-8.
[  +0.000667] EXT4-fs error (device sde): ext4_put_super:1204: comm wsl-bootstrap: Couldn't clean up the journal
[  +0.000777] EXT4-fs (sde): Remounting filesystem read-only
[  +0.596917] new mount options do not match the existing superblock, will be ignored
[  +0.000460] netlink: 'init': attribute type 4 has an invalid length.
[Apr29 07:20] tmpfs: Unknown parameter 'noswap'


==> etcd [22862f449c78] <==
{"level":"info","ts":"2025-04-29T07:21:20.367299Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"20.603063ms"}
{"level":"info","ts":"2025-04-29T07:21:20.413358Z","caller":"etcdserver/server.go:511","msg":"recovered v2 store from snapshot","snapshot-index":20002,"snapshot-size":"7.1 kB"}
{"level":"info","ts":"2025-04-29T07:21:20.413414Z","caller":"etcdserver/server.go:524","msg":"recovered v3 backend from snapshot","backend-size-bytes":1851392,"backend-size":"1.9 MB","backend-size-in-use-bytes":856064,"backend-size-in-use":"856 kB"}
{"level":"info","ts":"2025-04-29T07:21:20.489442Z","caller":"etcdserver/raft.go:540","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":20577}
{"level":"info","ts":"2025-04-29T07:21:20.489856Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2025-04-29T07:21:20.489919Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 2"}
{"level":"info","ts":"2025-04-29T07:21:20.489960Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [aec36adc501070cc], term: 2, commit: 20577, applied: 20002, lastindex: 20577, lastterm: 2]"}
{"level":"info","ts":"2025-04-29T07:21:20.490129Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2025-04-29T07:21:20.490493Z","caller":"membership/cluster.go:278","msg":"recovered/added member from store","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","recovered-remote-peer-id":"aec36adc501070cc","recovered-remote-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2025-04-29T07:21:20.490504Z","caller":"membership/cluster.go:287","msg":"set cluster version from store","cluster-version":"3.5"}
{"level":"warn","ts":"2025-04-29T07:21:20.491734Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2025-04-29T07:21:20.492728Z","caller":"mvcc/kvstore.go:346","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":16098}
{"level":"info","ts":"2025-04-29T07:21:20.494034Z","caller":"mvcc/kvstore.go:423","msg":"kvstore restored","current-rev":16446}
{"level":"info","ts":"2025-04-29T07:21:20.495887Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2025-04-29T07:21:20.497350Z","caller":"etcdserver/corrupt.go:96","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2025-04-29T07:21:20.497519Z","caller":"etcdserver/corrupt.go:177","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2025-04-29T07:21:20.497585Z","caller":"etcdserver/server.go:864","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.16","cluster-id":"fa54960ea34d58be","cluster-version":"3.5"}
{"level":"info","ts":"2025-04-29T07:21:20.497681Z","caller":"etcdserver/server.go:757","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2025-04-29T07:21:20.497853Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-04-29T07:21:20.498253Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2025-04-29T07:21:20.498352Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2025-04-29T07:21:20.498360Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2025-04-29T07:21:20.498927Z","caller":"embed/etcd.go:729","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-04-29T07:21:20.498987Z","caller":"embed/etcd.go:600","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-04-29T07:21:20.499060Z","caller":"embed/etcd.go:572","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-04-29T07:21:20.499307Z","caller":"embed/etcd.go:280","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2025-04-29T07:21:20.499344Z","caller":"embed/etcd.go:871","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2025-04-29T07:21:20.791304Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 2"}
{"level":"info","ts":"2025-04-29T07:21:20.791360Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 2"}
{"level":"info","ts":"2025-04-29T07:21:20.791433Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2025-04-29T07:21:20.791449Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 3"}
{"level":"info","ts":"2025-04-29T07:21:20.791455Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 3"}
{"level":"info","ts":"2025-04-29T07:21:20.791576Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 3"}
{"level":"info","ts":"2025-04-29T07:21:20.791809Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 3"}
{"level":"info","ts":"2025-04-29T07:21:20.795322Z","caller":"etcdserver/server.go:2140","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2025-04-29T07:21:20.795337Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-04-29T07:21:20.795362Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-04-29T07:21:20.795541Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-04-29T07:21:20.795566Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-04-29T07:21:20.797471Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-04-29T07:21:20.797476Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-04-29T07:21:20.797975Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2025-04-29T07:21:20.797978Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2025-04-29T07:29:40.974476Z","caller":"traceutil/trace.go:171","msg":"trace[1568019444] transaction","detail":"{read_only:false; response_revision:17105; number_of_response:1; }","duration":"204.388164ms","start":"2025-04-29T07:29:40.768597Z","end":"2025-04-29T07:29:40.972985Z","steps":["trace[1568019444] 'process raft request'  (duration: 204.314608ms)"],"step_count":1}
{"level":"info","ts":"2025-04-29T07:31:17.639272Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":16922}
{"level":"info","ts":"2025-04-29T07:31:17.651802Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":16922,"took":"12.18704ms","hash":2142337188,"current-db-size-bytes":2760704,"current-db-size":"2.8 MB","current-db-size-in-use-bytes":1433600,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2025-04-29T07:31:17.651836Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2142337188,"revision":16922,"compact-revision":16098}
{"level":"info","ts":"2025-04-29T07:36:15.969789Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":17185}
{"level":"info","ts":"2025-04-29T07:36:15.973198Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":17185,"took":"3.268946ms","hash":4203240021,"current-db-size-bytes":2760704,"current-db-size":"2.8 MB","current-db-size-in-use-bytes":1724416,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-04-29T07:36:15.973226Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":4203240021,"revision":17185,"compact-revision":16922}
{"level":"info","ts":"2025-04-29T07:36:34.857908Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2025-04-29T07:36:34.857983Z","caller":"embed/etcd.go:378","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2025-04-29T07:36:34.858122Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2025-04-29T07:36:34.858219Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2025-04-29T07:36:34.955174Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2025-04-29T07:36:34.955207Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"info","ts":"2025-04-29T07:36:34.955234Z","caller":"etcdserver/server.go:1543","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2025-04-29T07:36:34.967212Z","caller":"embed/etcd.go:582","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-04-29T07:36:34.967357Z","caller":"embed/etcd.go:587","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-04-29T07:36:34.967385Z","caller":"embed/etcd.go:380","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}


==> etcd [e058a0d39561] <==
{"level":"warn","ts":"2025-04-29T07:36:52.369838Z","caller":"embed/config.go:689","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2025-04-29T07:36:52.369900Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2025-04-29T07:36:52.369953Z","caller":"etcdmain/etcd.go:116","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"warn","ts":"2025-04-29T07:36:52.369966Z","caller":"embed/config.go:689","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2025-04-29T07:36:52.369971Z","caller":"embed/etcd.go:128","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2025-04-29T07:36:52.369988Z","caller":"embed/etcd.go:497","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-04-29T07:36:52.371593Z","caller":"embed/etcd.go:136","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2025-04-29T07:36:52.371787Z","caller":"embed/etcd.go:311","msg":"starting an etcd server","etcd-version":"3.5.16","git-sha":"f20bbad","go-version":"go1.22.7","go-os":"linux","go-arch":"amd64","max-cpu-set":16,"max-cpu-available":16,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2025-04-29T07:36:52.375140Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"2.967374ms"}
{"level":"info","ts":"2025-04-29T07:36:53.057140Z","caller":"etcdserver/server.go:511","msg":"recovered v2 store from snapshot","snapshot-index":20002,"snapshot-size":"7.1 kB"}
{"level":"info","ts":"2025-04-29T07:36:53.057198Z","caller":"etcdserver/server.go:524","msg":"recovered v3 backend from snapshot","backend-size-bytes":2760704,"backend-size":"2.8 MB","backend-size-in-use-bytes":1220608,"backend-size-in-use":"1.2 MB"}
{"level":"info","ts":"2025-04-29T07:36:53.256786Z","caller":"etcdserver/raft.go:540","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":21783}
{"level":"info","ts":"2025-04-29T07:36:53.263887Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2025-04-29T07:36:53.264190Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 3"}
{"level":"info","ts":"2025-04-29T07:36:53.264210Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [aec36adc501070cc], term: 3, commit: 21783, applied: 20002, lastindex: 21783, lastterm: 3]"}
{"level":"info","ts":"2025-04-29T07:36:53.264345Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2025-04-29T07:36:53.264358Z","caller":"membership/cluster.go:278","msg":"recovered/added member from store","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","recovered-remote-peer-id":"aec36adc501070cc","recovered-remote-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2025-04-29T07:36:53.264389Z","caller":"membership/cluster.go:287","msg":"set cluster version from store","cluster-version":"3.5"}
{"level":"warn","ts":"2025-04-29T07:36:53.268879Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2025-04-29T07:36:53.273125Z","caller":"mvcc/kvstore.go:346","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":17185}
{"level":"info","ts":"2025-04-29T07:36:53.274848Z","caller":"mvcc/kvstore.go:423","msg":"kvstore restored","current-rev":17446}
{"level":"info","ts":"2025-04-29T07:36:53.277386Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2025-04-29T07:36:53.279452Z","caller":"etcdserver/corrupt.go:96","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2025-04-29T07:36:53.279715Z","caller":"etcdserver/corrupt.go:177","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2025-04-29T07:36:53.279761Z","caller":"etcdserver/server.go:864","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.16","cluster-id":"fa54960ea34d58be","cluster-version":"3.5"}
{"level":"info","ts":"2025-04-29T07:36:53.279928Z","caller":"etcdserver/server.go:757","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2025-04-29T07:36:53.280047Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-04-29T07:36:53.280089Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2025-04-29T07:36:53.280134Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2025-04-29T07:36:53.280140Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2025-04-29T07:36:53.280898Z","caller":"embed/etcd.go:729","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-04-29T07:36:53.281086Z","caller":"embed/etcd.go:280","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2025-04-29T07:36:53.281117Z","caller":"embed/etcd.go:871","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2025-04-29T07:36:53.281197Z","caller":"embed/etcd.go:600","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-04-29T07:36:53.281214Z","caller":"embed/etcd.go:572","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-04-29T07:36:53.567082Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 3"}
{"level":"info","ts":"2025-04-29T07:36:53.567130Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 3"}
{"level":"info","ts":"2025-04-29T07:36:53.567180Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 3"}
{"level":"info","ts":"2025-04-29T07:36:53.567190Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 4"}
{"level":"info","ts":"2025-04-29T07:36:53.567195Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 4"}
{"level":"info","ts":"2025-04-29T07:36:53.567201Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 4"}
{"level":"info","ts":"2025-04-29T07:36:53.567205Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 4"}
{"level":"info","ts":"2025-04-29T07:36:53.571801Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-04-29T07:36:53.571988Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-04-29T07:36:53.571800Z","caller":"etcdserver/server.go:2140","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2025-04-29T07:36:53.572083Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-04-29T07:36:53.572313Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-04-29T07:36:53.572592Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-04-29T07:36:53.572844Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-04-29T07:36:53.573009Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2025-04-29T07:36:53.655260Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}


==> kernel <==
 07:44:19 up  1:29,  0 users,  load average: 0.10, 0.13, 0.09
Linux minikube 5.15.167.4-microsoft-standard-WSL2 #1 SMP Tue Nov 5 00:21:55 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [8657334eb8f0] <==
I0429 07:36:55.161714       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0429 07:36:55.161775       1 secure_serving.go:213] Serving securely on [::]:8443
I0429 07:36:55.161809       1 dynamic_serving_content.go:135] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I0429 07:36:55.161842       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0429 07:36:55.161907       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0429 07:36:55.162012       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I0429 07:36:55.162048       1 aggregator.go:169] waiting for initial CRD sync...
I0429 07:36:55.162121       1 local_available_controller.go:156] Starting LocalAvailability controller
I0429 07:36:55.162131       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I0429 07:36:55.162158       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I0429 07:36:55.162159       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0429 07:36:55.161812       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0429 07:36:55.162171       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I0429 07:36:55.162175       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0429 07:36:55.162407       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0429 07:36:55.162572       1 controller.go:78] Starting OpenAPI AggregationController
I0429 07:36:55.162702       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0429 07:36:55.163009       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I0429 07:36:55.163039       1 system_namespaces_controller.go:66] Starting system namespaces controller
I0429 07:36:55.163061       1 controller.go:142] Starting OpenAPI controller
I0429 07:36:55.163225       1 controller.go:90] Starting OpenAPI V3 controller
I0429 07:36:55.163255       1 naming_controller.go:294] Starting NamingConditionController
I0429 07:36:55.163307       1 establishing_controller.go:81] Starting EstablishingController
I0429 07:36:55.163342       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0429 07:36:55.163358       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0429 07:36:55.163378       1 crd_finalizer.go:269] Starting CRDFinalizer
I0429 07:36:55.163009       1 controller.go:119] Starting legacy_token_tracking_controller
I0429 07:36:55.163385       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I0429 07:36:55.173226       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I0429 07:36:55.173238       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I0429 07:36:55.235698       1 cluster_authentication_trust_controller.go:462] Starting cluster_authentication_trust_controller controller
I0429 07:36:55.235726       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0429 07:36:55.235741       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0429 07:36:55.236344       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0429 07:36:55.276226       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0429 07:36:55.276290       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0429 07:36:55.276298       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0429 07:36:55.276413       1 aggregator.go:171] initial CRD sync complete...
I0429 07:36:55.276530       1 autoregister_controller.go:144] Starting autoregister controller
I0429 07:36:55.276547       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0429 07:36:55.277417       1 shared_informer.go:320] Caches are synced for node_authorizer
I0429 07:36:55.358006       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0429 07:36:55.362581       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0429 07:36:55.362587       1 cache.go:39] Caches are synced for LocalAvailability controller
I0429 07:36:55.363402       1 shared_informer.go:320] Caches are synced for configmaps
I0429 07:36:55.369337       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0429 07:36:55.369362       1 policy_source.go:240] refreshing policies
I0429 07:36:55.455564       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0429 07:36:55.455644       1 cache.go:39] Caches are synced for autoregister controller
I0429 07:36:55.463962       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0429 07:36:55.467423       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
E0429 07:36:55.468906       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
E0429 07:36:56.085672       1 storage.go:479] "Unhandled Error" err="Address {10.244.0.11  0xc004df6930 0xc0031b4af0} isn't valid (pod ip(s) doesn't match endpoint ip, skipping: [] vs 10.244.0.11 (kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b-sxd7q))" logger="UnhandledError"
E0429 07:36:56.086861       1 storage.go:489] "Unhandled Error" err="Failed to find a valid address, skipping subset: &{[{10.244.0.11  0xc004df6930 0xc0031b4af0}] [] [{ 8000 TCP <nil>}]}" logger="UnhandledError"
I0429 07:36:56.167087       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0429 07:36:57.272282       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0429 07:36:58.564692       1 controller.go:615] quota admission added evaluator for: endpoints
I0429 07:36:58.714180       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0429 07:36:58.813237       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0429 07:36:58.914078       1 controller.go:615] quota admission added evaluator for: deployments.apps


==> kube-apiserver [ef441cc2f8c1] <==
W0429 07:36:40.698698       1 logging.go:55] [core] [Channel #37 SubChannel #38]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0429 07:36:40.756839       1 logging.go:55] [core] [Channel #31 SubChannel #32]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0429 07:36:42.842606       1 logging.go:55] [core] [Channel #7 SubChannel #8]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0429 07:36:42.853160       1 logging.go:55] [core] [Channel #100 SubChannel #101]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0429 07:36:42.866432       1 logging.go:55] [core] [Channel #85 SubChannel #86]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0429 07:36:43.341995       1 logging.go:55] [core] [Channel #169 SubChannel #170]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0429 07:36:43.368817       1 logging.go:55] [core] [Channel #127 SubChannel #128]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0429 07:36:43.388637       1 logging.go:55] [core] [Channel #79 SubChannel #80]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0429 07:36:43.413084       1 logging.go:55] [core] [Channel #157 SubChannel #158]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0429 07:36:43.425440       1 logging.go:55] [core] [Channel #106 SubChannel #107]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0429 07:36:43.443086       1 logging.go:55] [core] [Channel #73 SubChannel #74]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0429 07:36:43.463351       1 logging.go:55] [core] [Channel #103 SubChannel #104]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0429 07:36:43.513848       1 logging.go:55] [core] [Channel #2 SubChannel #3]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0429 07:36:43.536709       1 logging.go:55] [core] [Channel #46 SubChannel #47]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0429 07:36:43.570620       1 logging.go:55] [core] [Channel #166 SubChannel #167]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0429 07:36:43.612953       1 logging.go:55] [core] [Channel #10 SubChannel #11]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0429 07:36:43.636099       1 logging.go:55] [core] [Channel #15 SubChannel #18]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0429 07:36:43.702869       1 logging.go:55] [core] [Channel #52 SubChannel #53]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0429 07:36:43.722295       1 logging.go:55] [core] [Channel #58 SubChannel #59]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0429 07:36:43.757530       1 logging.go:55] [core] [Channel #139 SubChannel #140]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0429 07:36:43.758953       1 logging.go:55] [core] [Channel #115 SubChannel #116]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0429 07:36:43.783107       1 logging.go:55] [core] [Channel #88 SubChannel #89]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0429 07:36:43.793741       1 logging.go:55] [core] [Channel #142 SubChannel #143]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0429 07:36:43.800392       1 logging.go:55] [core] [Channel #175 SubChannel #176]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0429 07:36:43.814186       1 logging.go:55] [core] [Channel #121 SubChannel #122]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0429 07:36:43.843448       1 logging.go:55] [core] [Channel #1 SubChannel #4]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0429 07:36:43.853923       1 logging.go:55] [core] [Channel #61 SubChannel #62]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0429 07:36:43.878698       1 logging.go:55] [core] [Channel #22 SubChannel #23]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0429 07:36:43.878863       1 logging.go:55] [core] [Channel #178 SubChannel #179]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0429 07:36:43.885525       1 logging.go:55] [core] [Channel #112 SubChannel #113]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0429 07:36:43.918729       1 logging.go:55] [core] [Channel #94 SubChannel #95]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0429 07:36:43.935349       1 logging.go:55] [core] [Channel #49 SubChannel #50]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0429 07:36:43.970492       1 logging.go:55] [core] [Channel #151 SubChannel #152]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0429 07:36:44.063871       1 logging.go:55] [core] [Channel #130 SubChannel #131]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0429 07:36:44.097031       1 logging.go:55] [core] [Channel #28 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0429 07:36:44.135280       1 logging.go:55] [core] [Channel #145 SubChannel #146]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0429 07:36:44.150561       1 logging.go:55] [core] [Channel #133 SubChannel #134]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0429 07:36:44.187107       1 logging.go:55] [core] [Channel #97 SubChannel #98]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0429 07:36:44.298634       1 logging.go:55] [core] [Channel #148 SubChannel #149]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0429 07:36:44.385253       1 logging.go:55] [core] [Channel #172 SubChannel #173]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0429 07:36:44.386891       1 logging.go:55] [core] [Channel #124 SubChannel #125]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0429 07:36:44.453518       1 logging.go:55] [core] [Channel #118 SubChannel #119]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0429 07:36:44.454616       1 logging.go:55] [core] [Channel #67 SubChannel #68]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0429 07:36:44.507837       1 logging.go:55] [core] [Channel #70 SubChannel #71]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0429 07:36:44.553426       1 logging.go:55] [core] [Channel #64 SubChannel #65]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0429 07:36:44.565946       1 logging.go:55] [core] [Channel #25 SubChannel #26]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0429 07:36:44.575067       1 logging.go:55] [core] [Channel #55 SubChannel #56]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0429 07:36:44.591050       1 logging.go:55] [core] [Channel #160 SubChannel #161]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0429 07:36:44.644697       1 logging.go:55] [core] [Channel #37 SubChannel #38]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0429 07:36:44.736691       1 logging.go:55] [core] [Channel #76 SubChannel #77]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0429 07:36:44.843773       1 logging.go:55] [core] [Channel #154 SubChannel #155]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0429 07:36:44.854806       1 logging.go:55] [core] [Channel #34 SubChannel #35]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0429 07:36:44.862165       1 logging.go:55] [core] [Channel #91 SubChannel #92]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0429 07:36:44.919689       1 logging.go:55] [core] [Channel #82 SubChannel #83]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
E0429 07:36:44.957188       1 writers.go:123] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E0429 07:36:44.957211       1 finisher.go:175] "Unhandled Error" err="FinishRequest: post-timeout activity - time-elapsed: 43.668µs, panicked: false, err: context canceled, panic-reason: <nil>" logger="UnhandledError"
E0429 07:36:44.958407       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E0429 07:36:44.959632       1 writers.go:136] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
W0429 07:36:44.959819       1 logging.go:55] [core] [Channel #109 SubChannel #110]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
E0429 07:36:44.960969       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="3.898352ms" method="PUT" path="/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube" result=null


==> kube-controller-manager [0bbedf9173d5] <==
I0429 07:36:58.482118       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0429 07:36:58.482121       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I0429 07:36:58.482125       1 shared_informer.go:320] Caches are synced for cidrallocator
I0429 07:36:58.482183       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0429 07:36:58.483449       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I0429 07:36:58.487872       1 shared_informer.go:320] Caches are synced for disruption
I0429 07:36:58.489080       1 shared_informer.go:320] Caches are synced for PVC protection
I0429 07:36:58.491391       1 shared_informer.go:320] Caches are synced for crt configmap
I0429 07:36:58.492485       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I0429 07:36:58.494932       1 shared_informer.go:320] Caches are synced for ReplicationController
I0429 07:36:58.499281       1 shared_informer.go:320] Caches are synced for stateful set
I0429 07:36:58.500433       1 shared_informer.go:320] Caches are synced for certificate-csrapproving
I0429 07:36:58.501662       1 shared_informer.go:320] Caches are synced for TTL
I0429 07:36:58.503998       1 shared_informer.go:320] Caches are synced for persistent volume
I0429 07:36:58.506291       1 shared_informer.go:320] Caches are synced for TTL after finished
I0429 07:36:58.507601       1 shared_informer.go:320] Caches are synced for GC
I0429 07:36:58.517595       1 shared_informer.go:320] Caches are synced for cronjob
I0429 07:36:58.517643       1 shared_informer.go:320] Caches are synced for resource quota
I0429 07:36:58.517662       1 shared_informer.go:320] Caches are synced for HPA
I0429 07:36:58.517663       1 shared_informer.go:320] Caches are synced for daemon sets
I0429 07:36:58.517674       1 shared_informer.go:320] Caches are synced for expand
I0429 07:36:58.517721       1 shared_informer.go:320] Caches are synced for bootstrap_signer
I0429 07:36:58.517754       1 shared_informer.go:320] Caches are synced for PV protection
I0429 07:36:58.517762       1 shared_informer.go:320] Caches are synced for endpoint_slice
I0429 07:36:58.517754       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I0429 07:36:58.517787       1 shared_informer.go:320] Caches are synced for validatingadmissionpolicy-status
I0429 07:36:58.518026       1 shared_informer.go:320] Caches are synced for resource quota
I0429 07:36:58.522979       1 shared_informer.go:320] Caches are synced for garbage collector
I0429 07:36:58.523005       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0429 07:36:58.523010       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I0429 07:36:58.524085       1 shared_informer.go:320] Caches are synced for job
I0429 07:36:58.528076       1 shared_informer.go:320] Caches are synced for garbage collector
I0429 07:36:58.721369       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deployment-96b9d695" duration="258.5576ms"
I0429 07:36:58.722049       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deployment-96b9d695" duration="53.559µs"
I0429 07:36:59.326793       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deployment-96b9d695" duration="4.580647ms"
I0429 07:36:59.326839       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deployment-96b9d695" duration="19.634µs"
I0429 07:37:01.346261       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deployment-96b9d695" duration="4.933329ms"
I0429 07:37:01.346329       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deployment-96b9d695" duration="25.937µs"
I0429 07:37:05.220328       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deployment-96b9d695" duration="3.206988ms"
I0429 07:37:05.222782       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deployment-96b9d695" duration="23.283µs"
I0429 07:39:52.952431       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/todo-app-7d84777d8c" duration="79.345µs"
I0429 07:40:03.782867       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/todo-app-7d84777d8c" duration="73.417µs"
I0429 07:40:32.833643       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/todo-app-7d84777d8c" duration="2.403µs"
I0429 07:41:39.887651       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/todo-app-7d84777d8c" duration="12.697208ms"
I0429 07:41:39.893224       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/todo-app-7d84777d8c" duration="5.535973ms"
I0429 07:41:39.903274       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/todo-app-7d84777d8c" duration="10.021642ms"
I0429 07:41:39.903339       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/todo-app-7d84777d8c" duration="27.48µs"
I0429 07:41:43.575802       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/todo-app-7d84777d8c" duration="28.947µs"
I0429 07:41:50.562307       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/todo-app-7d84777d8c" duration="5.491µs"
I0429 07:42:10.141190       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0429 07:42:57.105115       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/todo-app-7d84777d8c" duration="14.269121ms"
I0429 07:42:57.110549       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/todo-app-7d84777d8c" duration="5.385674ms"
I0429 07:42:57.116647       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/todo-app-7d84777d8c" duration="6.054062ms"
I0429 07:42:57.116692       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/todo-app-7d84777d8c" duration="20.219µs"
I0429 07:43:00.746169       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/todo-app-7d84777d8c" duration="638.393µs"
I0429 07:43:14.782385       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/todo-app-7d84777d8c" duration="31.217µs"
I0429 07:43:32.615186       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/todo-app-7d84777d8c" duration="33.167µs"
I0429 07:43:45.616760       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/todo-app-7d84777d8c" duration="29.619µs"
I0429 07:44:02.449336       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/todo-app-7d84777d8c" duration="28.243µs"
I0429 07:44:16.451528       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/todo-app-7d84777d8c" duration="30.623µs"


==> kube-controller-manager [a3596411d9f2] <==
I0429 07:21:25.299289       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I0429 07:21:25.299293       1 shared_informer.go:320] Caches are synced for cidrallocator
I0429 07:21:25.299432       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0429 07:21:25.301434       1 shared_informer.go:320] Caches are synced for ReplicaSet
I0429 07:21:25.301857       1 shared_informer.go:320] Caches are synced for namespace
I0429 07:21:25.340898       1 shared_informer.go:320] Caches are synced for ephemeral
I0429 07:21:25.340917       1 shared_informer.go:320] Caches are synced for disruption
I0429 07:21:25.340940       1 shared_informer.go:320] Caches are synced for validatingadmissionpolicy-status
I0429 07:21:25.340974       1 shared_informer.go:320] Caches are synced for deployment
I0429 07:21:25.341259       1 shared_informer.go:320] Caches are synced for persistent volume
I0429 07:21:25.343501       1 shared_informer.go:320] Caches are synced for resource quota
I0429 07:21:25.354653       1 shared_informer.go:320] Caches are synced for garbage collector
I0429 07:21:25.745699       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deployment-96b9d695" duration="443.967878ms"
I0429 07:21:25.745778       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deployment-96b9d695" duration="27.281µs"
I0429 07:21:25.747913       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="446.179534ms"
I0429 07:21:25.747984       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b" duration="446.516932ms"
I0429 07:21:25.748115       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="446.425016ms"
I0429 07:21:25.751185       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="3.057923ms"
I0429 07:21:25.751251       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="36.654µs"
I0429 07:21:25.753479       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b" duration="5.481497ms"
I0429 07:21:25.753534       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b" duration="22.618µs"
I0429 07:21:25.756595       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="8.656761ms"
I0429 07:21:25.756667       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="32.232µs"
I0429 07:21:46.657532       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="6.032858ms"
I0429 07:21:46.657608       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="43.178µs"
I0429 07:21:47.536111       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="7.021997ms"
I0429 07:21:47.536278       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="52.985µs"
I0429 07:21:49.156402       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="30.018µs"
I0429 07:21:49.697304       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deployment-96b9d695" duration="4.386686ms"
I0429 07:21:49.697433       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deployment-96b9d695" duration="31.368µs"
I0429 07:21:51.718186       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deployment-96b9d695" duration="4.596055ms"
I0429 07:21:51.718291       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deployment-96b9d695" duration="25.221µs"
I0429 07:21:52.755895       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0429 07:21:53.735958       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deployment-96b9d695" duration="6.034821ms"
I0429 07:21:53.736030       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deployment-96b9d695" duration="38.257µs"
I0429 07:21:55.756835       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deployment-96b9d695" duration="6.842951ms"
I0429 07:21:55.756884       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deployment-96b9d695" duration="20.002µs"
I0429 07:22:00.800104       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="5.488985ms"
I0429 07:22:00.800296       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="21.582µs"
I0429 07:23:30.241629       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/todo-app-7d84777d8c" duration="14.424988ms"
I0429 07:23:30.245832       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/todo-app-7d84777d8c" duration="4.137175ms"
I0429 07:23:30.254801       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/todo-app-7d84777d8c" duration="8.916996ms"
I0429 07:23:30.254860       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/todo-app-7d84777d8c" duration="27.855µs"
I0429 07:23:33.984567       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/todo-app-7d84777d8c" duration="32.39µs"
I0429 07:23:44.523010       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/todo-app-7d84777d8c" duration="31.862µs"
I0429 07:23:59.526651       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/todo-app-7d84777d8c" duration="29.664µs"
I0429 07:24:13.375244       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/todo-app-7d84777d8c" duration="32.93µs"
I0429 07:24:29.371710       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/todo-app-7d84777d8c" duration="31.044µs"
I0429 07:24:43.215135       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/todo-app-7d84777d8c" duration="48.199µs"
I0429 07:25:14.054658       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/todo-app-7d84777d8c" duration="30.512µs"
I0429 07:25:26.058748       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/todo-app-7d84777d8c" duration="28.956µs"
I0429 07:25:56.121246       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0429 07:26:44.602309       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/todo-app-7d84777d8c" duration="115.864µs"
I0429 07:26:55.607130       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/todo-app-7d84777d8c" duration="30.443µs"
I0429 07:29:43.689919       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/todo-app-7d84777d8c" duration="125.146µs"
I0429 07:29:55.685741       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/todo-app-7d84777d8c" duration="98.31µs"
I0429 07:31:00.413907       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0429 07:34:47.603314       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/todo-app-7d84777d8c" duration="50.652µs"
I0429 07:34:53.660957       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0429 07:34:58.604747       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/todo-app-7d84777d8c" duration="55.435µs"


==> kube-proxy [4ba95890a455] <==
I0429 07:36:51.973412       1 server_linux.go:66] "Using iptables proxy"
E0429 07:36:52.461112       1 server.go:687] "Failed to retrieve node info" err="Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
I0429 07:36:55.370692       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0429 07:36:55.371233       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0429 07:36:55.571740       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0429 07:36:55.571816       1 server_linux.go:170] "Using iptables Proxier"
I0429 07:36:55.573945       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E0429 07:36:55.582881       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E0429 07:36:55.589639       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I0429 07:36:55.589744       1 server.go:497] "Version info" version="v1.32.0"
I0429 07:36:55.589750       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
E0429 07:36:55.655962       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E0429 07:36:55.666085       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I0429 07:36:55.666910       1 config.go:105] "Starting endpoint slice config controller"
I0429 07:36:55.666955       1 config.go:199] "Starting service config controller"
I0429 07:36:55.666973       1 shared_informer.go:313] Waiting for caches to sync for service config
I0429 07:36:55.666933       1 config.go:329] "Starting node config controller"
I0429 07:36:55.666991       1 shared_informer.go:313] Waiting for caches to sync for node config
I0429 07:36:55.666972       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0429 07:36:55.768021       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0429 07:36:55.768031       1 shared_informer.go:320] Caches are synced for node config
I0429 07:36:55.768043       1 shared_informer.go:320] Caches are synced for service config


==> kube-proxy [fd91d60f25ce] <==
I0429 07:21:24.877550       1 server_linux.go:66] "Using iptables proxy"
I0429 07:21:25.162014       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0429 07:21:25.162091       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0429 07:21:25.176078       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0429 07:21:25.176118       1 server_linux.go:170] "Using iptables Proxier"
I0429 07:21:25.177887       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E0429 07:21:25.183270       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E0429 07:21:25.188460       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I0429 07:21:25.188874       1 server.go:497] "Version info" version="v1.32.0"
I0429 07:21:25.188906       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
E0429 07:21:25.193760       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E0429 07:21:25.197377       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I0429 07:21:25.199431       1 config.go:199] "Starting service config controller"
I0429 07:21:25.199570       1 config.go:105] "Starting endpoint slice config controller"
I0429 07:21:25.199619       1 config.go:329] "Starting node config controller"
I0429 07:21:25.200653       1 shared_informer.go:313] Waiting for caches to sync for service config
I0429 07:21:25.200725       1 shared_informer.go:313] Waiting for caches to sync for node config
I0429 07:21:25.200766       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0429 07:21:25.301463       1 shared_informer.go:320] Caches are synced for service config
I0429 07:21:25.301504       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0429 07:21:25.301512       1 shared_informer.go:320] Caches are synced for node config


==> kube-scheduler [17352b3dce2e] <==
I0429 07:21:21.017278       1 serving.go:386] Generated self-signed cert in-memory
W0429 07:21:21.892387       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0429 07:21:21.892422       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0429 07:21:21.892429       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0429 07:21:21.892433       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0429 07:21:21.990373       1 server.go:166] "Starting Kubernetes Scheduler" version="v1.32.0"
I0429 07:21:21.990407       1 server.go:168] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0429 07:21:21.993236       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0429 07:21:21.993324       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0429 07:21:21.993343       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0429 07:21:21.995466       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0429 07:21:22.172592       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0429 07:36:34.960887       1 tlsconfig.go:258] "Shutting down DynamicServingCertificateController"
E0429 07:36:34.961005       1 run.go:72] "command failed" err="finished without leader elect"
I0429 07:36:34.961011       1 secure_serving.go:258] Stopped listening on 127.0.0.1:10259


==> kube-scheduler [5d20a6cba9b7] <==
I0429 07:36:54.191908       1 serving.go:386] Generated self-signed cert in-memory
I0429 07:36:55.371117       1 server.go:166] "Starting Kubernetes Scheduler" version="v1.32.0"
I0429 07:36:55.371166       1 server.go:168] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0429 07:36:55.469605       1 requestheader_controller.go:180] Starting RequestHeaderAuthRequestController
I0429 07:36:55.469661       1 shared_informer.go:313] Waiting for caches to sync for RequestHeaderAuthRequestController
I0429 07:36:55.469682       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0429 07:36:55.469710       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0429 07:36:55.469722       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0429 07:36:55.469739       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0429 07:36:55.469740       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0429 07:36:55.469913       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0429 07:36:55.569787       1 shared_informer.go:320] Caches are synced for RequestHeaderAuthRequestController
I0429 07:36:55.570203       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0429 07:36:55.570229       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Apr 29 07:36:55 minikube kubelet[1650]: E0429 07:36:55.081305    1650 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todo-app\" with ImagePullBackOff: \"Back-off pulling image \\\"akash5507/todo-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for akash5507/todo-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/todo-app-7d84777d8c-jhbvl" podUID="303d27fc-db71-4d8b-a677-7d55f7c17dd0"
Apr 29 07:36:55 minikube kubelet[1650]: I0429 07:36:55.188869    1650 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="dd6e784b69c403a5fbf88dfd2f7618cad72f8581b587d40d111015ebf0962a1b"
Apr 29 07:36:55 minikube kubelet[1650]: I0429 07:36:55.257740    1650 status_manager.go:890] "Failed to get status for pod" podUID="2b4b75c2a289008e0b381891e9683040" pod="kube-system/etcd-minikube" err="pods \"etcd-minikube\" is forbidden: User \"system:node:minikube\" cannot get resource \"pods\" in API group \"\" in the namespace \"kube-system\": no relationship found between node 'minikube' and this object"
Apr 29 07:36:55 minikube kubelet[1650]: E0429 07:36:55.258031    1650 reflector.go:166] "Unhandled Error" err="object-\"default\"/\"kube-root-ca.crt\": Failed to watch *v1.ConfigMap: configmaps \"kube-root-ca.crt\" is forbidden: User \"system:node:minikube\" cannot watch resource \"configmaps\" in API group \"\" in the namespace \"default\": no relationship found between node 'minikube' and this object" logger="UnhandledError"
Apr 29 07:36:55 minikube kubelet[1650]: E0429 07:36:55.258389    1650 reflector.go:166] "Unhandled Error" err="object-\"kube-system\"/\"kube-proxy\": Failed to watch *v1.ConfigMap: configmaps \"kube-proxy\" is forbidden: User \"system:node:minikube\" cannot watch resource \"configmaps\" in API group \"\" in the namespace \"kube-system\": no relationship found between node 'minikube' and this object" logger="UnhandledError"
Apr 29 07:36:55 minikube kubelet[1650]: E0429 07:36:55.258426    1650 reflector.go:166] "Unhandled Error" err="object-\"kube-system\"/\"coredns\": Failed to watch *v1.ConfigMap: configmaps \"coredns\" is forbidden: User \"system:node:minikube\" cannot watch resource \"configmaps\" in API group \"\" in the namespace \"kube-system\": no relationship found between node 'minikube' and this object" logger="UnhandledError"
Apr 29 07:36:56 minikube kubelet[1650]: E0429 07:36:56.273888    1650 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todo-app\" with ImagePullBackOff: \"Back-off pulling image \\\"akash5507/todo-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for akash5507/todo-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/todo-app-7d84777d8c-jhbvl" podUID="303d27fc-db71-4d8b-a677-7d55f7c17dd0"
Apr 29 07:37:07 minikube kubelet[1650]: E0429 07:37:07.769125    1650 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todo-app\" with ImagePullBackOff: \"Back-off pulling image \\\"akash5507/todo-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for akash5507/todo-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/todo-app-7d84777d8c-jhbvl" podUID="303d27fc-db71-4d8b-a677-7d55f7c17dd0"
Apr 29 07:37:08 minikube kubelet[1650]: I0429 07:37:08.768441    1650 scope.go:117] "RemoveContainer" containerID="dea086a0a41e914df5df4e64caef0e6f7cf6d5c9eb2f943806c29e6abaa553b4"
Apr 29 07:37:13 minikube kubelet[1650]: I0429 07:37:13.967894    1650 scope.go:117] "RemoveContainer" containerID="a0f740e8cb7e58937857aeb066bdfc049ab71f621283d5d6afcc0981fbc2e3b1"
Apr 29 07:37:18 minikube kubelet[1650]: E0429 07:37:18.768753    1650 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todo-app\" with ImagePullBackOff: \"Back-off pulling image \\\"akash5507/todo-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for akash5507/todo-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/todo-app-7d84777d8c-jhbvl" podUID="303d27fc-db71-4d8b-a677-7d55f7c17dd0"
Apr 29 07:37:31 minikube kubelet[1650]: E0429 07:37:31.769221    1650 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todo-app\" with ImagePullBackOff: \"Back-off pulling image \\\"akash5507/todo-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for akash5507/todo-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/todo-app-7d84777d8c-jhbvl" podUID="303d27fc-db71-4d8b-a677-7d55f7c17dd0"
Apr 29 07:37:43 minikube kubelet[1650]: E0429 07:37:43.601412    1650 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todo-app\" with ImagePullBackOff: \"Back-off pulling image \\\"akash5507/todo-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for akash5507/todo-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/todo-app-7d84777d8c-jhbvl" podUID="303d27fc-db71-4d8b-a677-7d55f7c17dd0"
Apr 29 07:37:55 minikube kubelet[1650]: E0429 07:37:55.599846    1650 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todo-app\" with ImagePullBackOff: \"Back-off pulling image \\\"akash5507/todo-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for akash5507/todo-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/todo-app-7d84777d8c-jhbvl" podUID="303d27fc-db71-4d8b-a677-7d55f7c17dd0"
Apr 29 07:38:09 minikube kubelet[1650]: E0429 07:38:09.437587    1650 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todo-app\" with ImagePullBackOff: \"Back-off pulling image \\\"akash5507/todo-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for akash5507/todo-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/todo-app-7d84777d8c-jhbvl" podUID="303d27fc-db71-4d8b-a677-7d55f7c17dd0"
Apr 29 07:38:23 minikube kubelet[1650]: E0429 07:38:23.437605    1650 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todo-app\" with ImagePullBackOff: \"Back-off pulling image \\\"akash5507/todo-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for akash5507/todo-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/todo-app-7d84777d8c-jhbvl" podUID="303d27fc-db71-4d8b-a677-7d55f7c17dd0"
Apr 29 07:38:34 minikube kubelet[1650]: E0429 07:38:34.279661    1650 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todo-app\" with ImagePullBackOff: \"Back-off pulling image \\\"akash5507/todo-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for akash5507/todo-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/todo-app-7d84777d8c-jhbvl" podUID="303d27fc-db71-4d8b-a677-7d55f7c17dd0"
Apr 29 07:38:47 minikube kubelet[1650]: E0429 07:38:47.279603    1650 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todo-app\" with ImagePullBackOff: \"Back-off pulling image \\\"akash5507/todo-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for akash5507/todo-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/todo-app-7d84777d8c-jhbvl" podUID="303d27fc-db71-4d8b-a677-7d55f7c17dd0"
Apr 29 07:38:58 minikube kubelet[1650]: E0429 07:38:58.279706    1650 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todo-app\" with ImagePullBackOff: \"Back-off pulling image \\\"akash5507/todo-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for akash5507/todo-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/todo-app-7d84777d8c-jhbvl" podUID="303d27fc-db71-4d8b-a677-7d55f7c17dd0"
Apr 29 07:39:13 minikube kubelet[1650]: E0429 07:39:13.099987    1650 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todo-app\" with ImagePullBackOff: \"Back-off pulling image \\\"akash5507/todo-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for akash5507/todo-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/todo-app-7d84777d8c-jhbvl" podUID="303d27fc-db71-4d8b-a677-7d55f7c17dd0"
Apr 29 07:39:27 minikube kubelet[1650]: E0429 07:39:27.102484    1650 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todo-app\" with ImagePullBackOff: \"Back-off pulling image \\\"akash5507/todo-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for akash5507/todo-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/todo-app-7d84777d8c-jhbvl" podUID="303d27fc-db71-4d8b-a677-7d55f7c17dd0"
Apr 29 07:39:41 minikube kubelet[1650]: E0429 07:39:41.876673    1650 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for akash5507/todo-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="akash5507/todo-app:latest"
Apr 29 07:39:41 minikube kubelet[1650]: E0429 07:39:41.876720    1650 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for akash5507/todo-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="akash5507/todo-app:latest"
Apr 29 07:39:41 minikube kubelet[1650]: E0429 07:39:41.876805    1650 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:todo-app,Image:akash5507/todo-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vrpb8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod todo-app-7d84777d8c-jhbvl_default(303d27fc-db71-4d8b-a677-7d55f7c17dd0): ErrImagePull: Error response from daemon: pull access denied for akash5507/todo-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Apr 29 07:39:41 minikube kubelet[1650]: E0429 07:39:41.877975    1650 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todo-app\" with ErrImagePull: \"Error response from daemon: pull access denied for akash5507/todo-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/todo-app-7d84777d8c-jhbvl" podUID="303d27fc-db71-4d8b-a677-7d55f7c17dd0"
Apr 29 07:39:52 minikube kubelet[1650]: E0429 07:39:52.940080    1650 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todo-app\" with ImagePullBackOff: \"Back-off pulling image \\\"akash5507/todo-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for akash5507/todo-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/todo-app-7d84777d8c-jhbvl" podUID="303d27fc-db71-4d8b-a677-7d55f7c17dd0"
Apr 29 07:40:03 minikube kubelet[1650]: E0429 07:40:03.774735    1650 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todo-app\" with ImagePullBackOff: \"Back-off pulling image \\\"akash5507/todo-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for akash5507/todo-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/todo-app-7d84777d8c-jhbvl" podUID="303d27fc-db71-4d8b-a677-7d55f7c17dd0"
Apr 29 07:40:15 minikube kubelet[1650]: E0429 07:40:15.775011    1650 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todo-app\" with ImagePullBackOff: \"Back-off pulling image \\\"akash5507/todo-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for akash5507/todo-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/todo-app-7d84777d8c-jhbvl" podUID="303d27fc-db71-4d8b-a677-7d55f7c17dd0"
Apr 29 07:40:30 minikube kubelet[1650]: E0429 07:40:30.774360    1650 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todo-app\" with ImagePullBackOff: \"Back-off pulling image \\\"akash5507/todo-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for akash5507/todo-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/todo-app-7d84777d8c-jhbvl" podUID="303d27fc-db71-4d8b-a677-7d55f7c17dd0"
Apr 29 07:40:33 minikube kubelet[1650]: I0429 07:40:33.499591    1650 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"kube-api-access-vrpb8\" (UniqueName: \"kubernetes.io/projected/303d27fc-db71-4d8b-a677-7d55f7c17dd0-kube-api-access-vrpb8\") pod \"303d27fc-db71-4d8b-a677-7d55f7c17dd0\" (UID: \"303d27fc-db71-4d8b-a677-7d55f7c17dd0\") "
Apr 29 07:40:33 minikube kubelet[1650]: I0429 07:40:33.504663    1650 operation_generator.go:780] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/303d27fc-db71-4d8b-a677-7d55f7c17dd0-kube-api-access-vrpb8" (OuterVolumeSpecName: "kube-api-access-vrpb8") pod "303d27fc-db71-4d8b-a677-7d55f7c17dd0" (UID: "303d27fc-db71-4d8b-a677-7d55f7c17dd0"). InnerVolumeSpecName "kube-api-access-vrpb8". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Apr 29 07:40:33 minikube kubelet[1650]: I0429 07:40:33.600453    1650 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-vrpb8\" (UniqueName: \"kubernetes.io/projected/303d27fc-db71-4d8b-a677-7d55f7c17dd0-kube-api-access-vrpb8\") on node \"minikube\" DevicePath \"\""
Apr 29 07:40:34 minikube kubelet[1650]: I0429 07:40:34.610652    1650 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="303d27fc-db71-4d8b-a677-7d55f7c17dd0" path="/var/lib/kubelet/pods/303d27fc-db71-4d8b-a677-7d55f7c17dd0/volumes"
Apr 29 07:41:39 minikube kubelet[1650]: I0429 07:41:39.967300    1650 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-755nf\" (UniqueName: \"kubernetes.io/projected/8545748c-5f4c-45fb-806e-070de4ad851f-kube-api-access-755nf\") pod \"todo-app-7d84777d8c-pqzct\" (UID: \"8545748c-5f4c-45fb-806e-070de4ad851f\") " pod="default/todo-app-7d84777d8c-pqzct"
Apr 29 07:41:43 minikube kubelet[1650]: E0429 07:41:43.235396    1650 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for akash5507/todo-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="akash5507/todo-app:latest"
Apr 29 07:41:43 minikube kubelet[1650]: E0429 07:41:43.235438    1650 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for akash5507/todo-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="akash5507/todo-app:latest"
Apr 29 07:41:43 minikube kubelet[1650]: E0429 07:41:43.235497    1650 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:todo-app,Image:akash5507/todo-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-755nf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod todo-app-7d84777d8c-pqzct_default(8545748c-5f4c-45fb-806e-070de4ad851f): ErrImagePull: Error response from daemon: pull access denied for akash5507/todo-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Apr 29 07:41:43 minikube kubelet[1650]: E0429 07:41:43.237138    1650 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todo-app\" with ErrImagePull: \"Error response from daemon: pull access denied for akash5507/todo-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/todo-app-7d84777d8c-pqzct" podUID="8545748c-5f4c-45fb-806e-070de4ad851f"
Apr 29 07:41:43 minikube kubelet[1650]: E0429 07:41:43.568287    1650 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todo-app\" with ImagePullBackOff: \"Back-off pulling image \\\"akash5507/todo-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for akash5507/todo-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/todo-app-7d84777d8c-pqzct" podUID="8545748c-5f4c-45fb-806e-070de4ad851f"
Apr 29 07:41:51 minikube kubelet[1650]: I0429 07:41:51.055253    1650 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"kube-api-access-755nf\" (UniqueName: \"kubernetes.io/projected/8545748c-5f4c-45fb-806e-070de4ad851f-kube-api-access-755nf\") pod \"8545748c-5f4c-45fb-806e-070de4ad851f\" (UID: \"8545748c-5f4c-45fb-806e-070de4ad851f\") "
Apr 29 07:41:51 minikube kubelet[1650]: I0429 07:41:51.056466    1650 operation_generator.go:780] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/8545748c-5f4c-45fb-806e-070de4ad851f-kube-api-access-755nf" (OuterVolumeSpecName: "kube-api-access-755nf") pod "8545748c-5f4c-45fb-806e-070de4ad851f" (UID: "8545748c-5f4c-45fb-806e-070de4ad851f"). InnerVolumeSpecName "kube-api-access-755nf". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Apr 29 07:41:51 minikube kubelet[1650]: I0429 07:41:51.156321    1650 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-755nf\" (UniqueName: \"kubernetes.io/projected/8545748c-5f4c-45fb-806e-070de4ad851f-kube-api-access-755nf\") on node \"minikube\" DevicePath \"\""
Apr 29 07:41:52 minikube kubelet[1650]: I0429 07:41:52.283720    1650 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="8545748c-5f4c-45fb-806e-070de4ad851f" path="/var/lib/kubelet/pods/8545748c-5f4c-45fb-806e-070de4ad851f/volumes"
Apr 29 07:42:57 minikube kubelet[1650]: I0429 07:42:57.124949    1650 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-4rvtd\" (UniqueName: \"kubernetes.io/projected/1eff6d31-4edf-4813-8d4b-515b16eff72a-kube-api-access-4rvtd\") pod \"todo-app-7d84777d8c-pddwn\" (UID: \"1eff6d31-4edf-4813-8d4b-515b16eff72a\") " pod="default/todo-app-7d84777d8c-pddwn"
Apr 29 07:43:00 minikube kubelet[1650]: E0429 07:43:00.620321    1650 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for akash5507/todo-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="akash5507/todo-app:latest"
Apr 29 07:43:00 minikube kubelet[1650]: E0429 07:43:00.620375    1650 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for akash5507/todo-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="akash5507/todo-app:latest"
Apr 29 07:43:00 minikube kubelet[1650]: E0429 07:43:00.620441    1650 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:todo-app,Image:akash5507/todo-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4rvtd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod todo-app-7d84777d8c-pddwn_default(1eff6d31-4edf-4813-8d4b-515b16eff72a): ErrImagePull: Error response from daemon: pull access denied for akash5507/todo-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Apr 29 07:43:00 minikube kubelet[1650]: E0429 07:43:00.621775    1650 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todo-app\" with ErrImagePull: \"Error response from daemon: pull access denied for akash5507/todo-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/todo-app-7d84777d8c-pddwn" podUID="1eff6d31-4edf-4813-8d4b-515b16eff72a"
Apr 29 07:43:00 minikube kubelet[1650]: E0429 07:43:00.737981    1650 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todo-app\" with ImagePullBackOff: \"Back-off pulling image \\\"akash5507/todo-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for akash5507/todo-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/todo-app-7d84777d8c-pddwn" podUID="1eff6d31-4edf-4813-8d4b-515b16eff72a"
Apr 29 07:43:17 minikube kubelet[1650]: E0429 07:43:17.648782    1650 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for akash5507/todo-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="akash5507/todo-app:latest"
Apr 29 07:43:17 minikube kubelet[1650]: E0429 07:43:17.648827    1650 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for akash5507/todo-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="akash5507/todo-app:latest"
Apr 29 07:43:17 minikube kubelet[1650]: E0429 07:43:17.648905    1650 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:todo-app,Image:akash5507/todo-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4rvtd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod todo-app-7d84777d8c-pddwn_default(1eff6d31-4edf-4813-8d4b-515b16eff72a): ErrImagePull: Error response from daemon: pull access denied for akash5507/todo-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Apr 29 07:43:17 minikube kubelet[1650]: E0429 07:43:17.650540    1650 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todo-app\" with ErrImagePull: \"Error response from daemon: pull access denied for akash5507/todo-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/todo-app-7d84777d8c-pddwn" podUID="1eff6d31-4edf-4813-8d4b-515b16eff72a"
Apr 29 07:43:32 minikube kubelet[1650]: E0429 07:43:32.606080    1650 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todo-app\" with ImagePullBackOff: \"Back-off pulling image \\\"akash5507/todo-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for akash5507/todo-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/todo-app-7d84777d8c-pddwn" podUID="1eff6d31-4edf-4813-8d4b-515b16eff72a"
Apr 29 07:43:48 minikube kubelet[1650]: E0429 07:43:48.516449    1650 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for akash5507/todo-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="akash5507/todo-app:latest"
Apr 29 07:43:48 minikube kubelet[1650]: E0429 07:43:48.516499    1650 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for akash5507/todo-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="akash5507/todo-app:latest"
Apr 29 07:43:48 minikube kubelet[1650]: E0429 07:43:48.516575    1650 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:todo-app,Image:akash5507/todo-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4rvtd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod todo-app-7d84777d8c-pddwn_default(1eff6d31-4edf-4813-8d4b-515b16eff72a): ErrImagePull: Error response from daemon: pull access denied for akash5507/todo-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Apr 29 07:43:48 minikube kubelet[1650]: E0429 07:43:48.517778    1650 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todo-app\" with ErrImagePull: \"Error response from daemon: pull access denied for akash5507/todo-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/todo-app-7d84777d8c-pddwn" podUID="1eff6d31-4edf-4813-8d4b-515b16eff72a"
Apr 29 07:44:02 minikube kubelet[1650]: E0429 07:44:02.443935    1650 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todo-app\" with ImagePullBackOff: \"Back-off pulling image \\\"akash5507/todo-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for akash5507/todo-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/todo-app-7d84777d8c-pddwn" podUID="1eff6d31-4edf-4813-8d4b-515b16eff72a"
Apr 29 07:44:16 minikube kubelet[1650]: E0429 07:44:16.444524    1650 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todo-app\" with ImagePullBackOff: \"Back-off pulling image \\\"akash5507/todo-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for akash5507/todo-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/todo-app-7d84777d8c-pddwn" podUID="1eff6d31-4edf-4813-8d4b-515b16eff72a"


==> kubernetes-dashboard [4ec195fe43d9] <==
2025/04/29 07:22:00 Using namespace: kubernetes-dashboard
2025/04/29 07:22:00 Using in-cluster config to connect to apiserver
2025/04/29 07:22:00 Using secret token for csrf signing
2025/04/29 07:22:00 Initializing csrf token from kubernetes-dashboard-csrf secret
2025/04/29 07:22:00 Empty token. Generating and storing in a secret kubernetes-dashboard-csrf
2025/04/29 07:22:00 Successful initial request to the apiserver, version: v1.32.0
2025/04/29 07:22:00 Generating JWE encryption key
2025/04/29 07:22:00 New synchronizer has been registered: kubernetes-dashboard-key-holder-kubernetes-dashboard. Starting
2025/04/29 07:22:00 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2025/04/29 07:22:00 Initializing JWE encryption key from synchronized object
2025/04/29 07:22:00 Creating in-cluster Sidecar client
2025/04/29 07:22:00 Successful request to sidecar
2025/04/29 07:22:00 Serving insecurely on HTTP port: 9090
2025/04/29 07:22:00 Starting overwatch


==> kubernetes-dashboard [bbcb6f8b86f3] <==
2025/04/29 07:36:54 Starting overwatch
2025/04/29 07:36:54 Using namespace: kubernetes-dashboard
2025/04/29 07:36:54 Using in-cluster config to connect to apiserver
2025/04/29 07:36:54 Using secret token for csrf signing
2025/04/29 07:36:54 Initializing csrf token from kubernetes-dashboard-csrf secret
2025/04/29 07:36:55 Successful initial request to the apiserver, version: v1.32.0
2025/04/29 07:36:55 Generating JWE encryption key
2025/04/29 07:36:55 New synchronizer has been registered: kubernetes-dashboard-key-holder-kubernetes-dashboard. Starting
2025/04/29 07:36:55 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2025/04/29 07:36:56 Initializing JWE encryption key from synchronized object
2025/04/29 07:36:56 Creating in-cluster Sidecar client
2025/04/29 07:36:56 Serving insecurely on HTTP port: 9090
2025/04/29 07:36:56 Metric client health check failed: the server is currently unable to handle the request (get services dashboard-metrics-scraper). Retrying in 30 seconds.
2025/04/29 07:37:25 Successful request to sidecar


==> storage-provisioner [a94645fe2069] <==
I0429 07:37:08.860693       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0429 07:37:08.865826       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0429 07:37:08.865866       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0429 07:37:26.255369       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0429 07:37:26.255474       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_ac644033-07a6-4f65-b820-3ba3cdc0f9f2!
I0429 07:37:26.255564       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"ec326b59-1778-4638-b57d-40a9416e79f7", APIVersion:"v1", ResourceVersion:"17636", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_ac644033-07a6-4f65-b820-3ba3cdc0f9f2 became leader
I0429 07:37:26.356199       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_ac644033-07a6-4f65-b820-3ba3cdc0f9f2!


==> storage-provisioner [dea086a0a41e] <==
I0429 07:36:51.859902       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0429 07:36:51.864906       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: connect: connection refused

